{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf5903a",
   "metadata": {},
   "source": [
    "# TNN (Tensor Neural Network) 教程\n",
    "\n",
    "## 张量神经网络：高维偏微分方程求解器\n",
    "\n",
    "本教程将介绍如何使用**张量神经网络（Tensor Neural Network, TNN）**求解高维偏微分方程问题。\n",
    "\n",
    "### 什么是TNN？\n",
    "\n",
    "TNN是一种专门用于高维PDE求解的深度学习方法，通过**张量分解**的思想将高维函数表示为低维函数的乘积之和：\n",
    "\n",
    "$$\n",
    "u(x_1, \\ldots, x_d) \\approx \\sum_{i=1}^{p} \\alpha_i \\prod_{k=1}^{d} \\phi_k^{(i)}(x_k)\n",
    "$$\n",
    "\n",
    "### 为什么使用TNN？\n",
    "\n",
    "传统数值方法面临**维数灾难**：\n",
    "- 有限差分/有限元：需要 $O(n^d)$ 个网格点\n",
    "- 蒙特卡洛：收敛速度慢\n",
    "\n",
    "TNN的优势：\n",
    "- ✅ 参数量仅为 $O(d \\cdot n \\cdot p)$，线性增长\n",
    "- ✅ 自然利用可分离结构\n",
    "- ✅ 易于强制边界条件\n",
    "- ✅ 高效的梯度计算\n",
    "\n",
    "### 教程内容\n",
    "\n",
    "1. 实现TNN基本组件\n",
    "2. 求解2D Poisson方程\n",
    "3. 可视化结果\n",
    "4. 扩展到3D问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3cd6a",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库\n",
    "\n",
    "我们需要以下库：\n",
    "- **PyTorch**: 构建神经网络\n",
    "- **NumPy**: 数值计算\n",
    "- **Matplotlib**: 可视化结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f044cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置数据类型和设备\n",
    "dtype = torch.double\n",
    "device = 'cpu'  # 如果有GPU可以改为'cuda'\n",
    "\n",
    "pi = np.pi\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a40b79a",
   "metadata": {},
   "source": [
    "## 2. 数学基础：张量分解\n",
    "\n",
    "### 张量分解表示\n",
    "\n",
    "TNN的核心是将高维函数 $u(x_1, \\ldots, x_d)$ 表示为：\n",
    "\n",
    "$$\n",
    "u(x_1, \\ldots, x_d) = \\sum_{i=1}^{p} \\alpha_i \\prod_{k=1}^{d} \\phi_k^{(i)}(x_k)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $d$: 问题维度\n",
    "- $p$: 张量秩（rank），控制表示能力\n",
    "- $\\alpha_i$: 缩放系数\n",
    "- $\\phi_k^{(i)}(x_k)$: 第 $k$ 维的第 $i$ 个基函数\n",
    "\n",
    "### 为什么这样表示有效？\n",
    "\n",
    "许多物理问题具有**可分离性**。例如：\n",
    "\n",
    "$$\n",
    "u(x, y) = \\sin(\\pi x) \\cdot \\sin(\\pi y) = \\phi_1(x) \\cdot \\phi_2(y)\n",
    "$$\n",
    "\n",
    "TNN通过学习这些基函数 $\\phi_k^{(i)}$ 来逼近解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dd6f16",
   "metadata": {},
   "source": [
    "## 3. TNN基本组件实现\n",
    "\n",
    "### 3.1 边界条件函数\n",
    "\n",
    "对于齐次Dirichlet边界条件 $u=0$ on $\\partial\\Omega$，我们使用强制函数：\n",
    "\n",
    "$$\n",
    "u(x) = \\text{bd}(x) \\cdot \\text{NN}(x)\n",
    "$$\n",
    "\n",
    "对于1D区间 $[a, b]$，常用的边界函数是：\n",
    "$$\n",
    "\\text{bd}(x) = (x-a)(b-x)\n",
    "$$\n",
    "\n",
    "这保证了 $\\text{bd}(a) = \\text{bd}(b) = 0$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d918e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define boundary condition function for domain [a, b]\n",
    "def bd(x, a=-1, b=1):\n",
    "    \"\"\"\n",
    "    Boundary condition function: (x-a)(b-x)\n",
    "    Ensures u=0 at x=a and x=b\n",
    "    \"\"\"\n",
    "    return (x - a) * (b - x)\n",
    "\n",
    "def grad_bd(x, a=-1, b=1):\n",
    "    \"\"\"\n",
    "    Gradient of boundary condition function: -2x + a + b\n",
    "    \"\"\"\n",
    "    return -2*x + a + b\n",
    "\n",
    "def grad_grad_bd(x):\n",
    "    \"\"\"\n",
    "    Second derivative of boundary condition: -2\n",
    "    \"\"\"\n",
    "    return -2 * torch.ones_like(x)\n",
    "\n",
    "# Test the boundary function\n",
    "x_test = torch.linspace(-1, 1, 100, dtype=dtype)\n",
    "bd_test = bd(x_test)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_test, bd_test, 'b-', linewidth=2, label='bd(x) = (x+1)(1-x)')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=-1, color='r', linestyle='--', alpha=0.3, label='Boundary')\n",
    "plt.axvline(x=1, color='r', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('bd(x)', fontsize=12)\n",
    "plt.title('Boundary Condition Function', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_test, grad_bd(x_test), 'g-', linewidth=2, label=\"bd'(x)\")\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel(\"bd'(x)\", fontsize=12)\n",
    "plt.title('Gradient of Boundary Function', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Boundary functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea375b6",
   "metadata": {},
   "source": [
    "### 3.2 TNN线性层 (TNN_Linear)\n",
    "\n",
    "TNN的关键是**批量处理多个维度**。每个维度都有自己的神经网络，但我们可以并行计算它们。\n",
    "\n",
    "**形状说明**:\n",
    "- 输入: `[dim, n_in, N]` - dim个维度，每个有N个采样点\n",
    "- 权重: `[dim, n_out, n_in]` - 每个维度有自己的权重矩阵\n",
    "- 输出: `[dim, n_out, N]`\n",
    "\n",
    "这比分别处理每个维度要高效得多！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3a1f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNN_Linear(nn.Module):\n",
    "    \"\"\"\n",
    "    Batch linear transformation for TNN\n",
    "    \n",
    "    Applies: y = Wx + b for each dimension simultaneously\n",
    "    \n",
    "    Args:\n",
    "        dim: number of dimensions\n",
    "        out_features: output dimension (n_out)\n",
    "        in_features: input dimension (n_in)\n",
    "        bias: whether to include bias term\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, out_features, in_features, bias=True):\n",
    "        super(TNN_Linear, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.out_features = out_features\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        # Weight for each dimension: [dim, out_features, in_features]\n",
    "        self.weight = nn.Parameter(torch.empty((dim, out_features, in_features), dtype=dtype))\n",
    "        \n",
    "        # Bias for each dimension (optional): [dim, out_features, 1]\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty((dim, out_features, 1), dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor [dim, in_features, N]\n",
    "        \n",
    "        Returns:\n",
    "            output tensor [dim, out_features, N]\n",
    "        \"\"\"\n",
    "        if self.bias is None:\n",
    "            return self.weight @ x  # Batch matrix multiplication\n",
    "        else:\n",
    "            return self.weight @ x + self.bias\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'dim={self.dim}, in_features={self.in_features}, out_features={self.out_features}'\n",
    "\n",
    "# Test TNN_Linear\n",
    "dim_test = 2\n",
    "in_feat = 3\n",
    "out_feat = 5\n",
    "N_test = 10\n",
    "\n",
    "tnn_linear = TNN_Linear(dim_test, out_feat, in_feat, bias=True)\n",
    "x_test = torch.randn(dim_test, in_feat, N_test, dtype=dtype)\n",
    "y_test = tnn_linear(x_test)\n",
    "\n",
    "print(f\"✓ TNN_Linear created!\")\n",
    "print(f\"  Input shape: {x_test.shape}\")\n",
    "print(f\"  Output shape: {y_test.shape}\")\n",
    "print(f\"  Weight shape: {tnn_linear.weight.shape}\")\n",
    "if tnn_linear.bias is not None:\n",
    "    print(f\"  Bias shape: {tnn_linear.bias.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ce02f",
   "metadata": {},
   "source": [
    "### 3.3 激活函数：TNN_Sin\n",
    "\n",
    "我们使用 $\\sin(x)$ 作为激活函数，因为：\n",
    "1. **无限次可微**: 适合求解需要高阶导数的PDE\n",
    "2. **周期性**: 适合表示振荡解\n",
    "3. **导数简单**: $\\frac{d}{dx}\\sin(x) = \\cos(x)$, $\\frac{d^2}{dx^2}\\sin(x) = -\\sin(x)$\n",
    "\n",
    "为了高效计算梯度，我们需要实现**激活函数及其导数**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNN_Sin(nn.Module):\n",
    "    \"\"\"\n",
    "    Sin activation function with derivatives\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute sin(x)\"\"\"\n",
    "        return torch.sin(x)\n",
    "    \n",
    "    def grad(self, x):\n",
    "        \"\"\"Compute first derivative: cos(x)\"\"\"\n",
    "        return torch.cos(x)\n",
    "    \n",
    "    def grad_grad(self, x):\n",
    "        \"\"\"Compute second derivative: -sin(x)\"\"\"\n",
    "        return -torch.sin(x)\n",
    "\n",
    "# Visualize the activation function and its derivatives\n",
    "x_range = torch.linspace(-2*pi, 2*pi, 200, dtype=dtype)\n",
    "activation = TNN_Sin()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# sin(x)\n",
    "axes[0].plot(x_range, activation(x_range), 'b-', linewidth=2)\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].set_xlabel('x', fontsize=12)\n",
    "axes[0].set_ylabel('sin(x)', fontsize=12)\n",
    "axes[0].set_title('Activation: sin(x)', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# cos(x)\n",
    "axes[1].plot(x_range, activation.grad(x_range), 'g-', linewidth=2)\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_ylabel('cos(x)', fontsize=12)\n",
    "axes[1].set_title('First Derivative: cos(x)', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# -sin(x)\n",
    "axes[2].plot(x_range, activation.grad_grad(x_range), 'r-', linewidth=2)\n",
    "axes[2].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[2].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[2].set_xlabel('x', fontsize=12)\n",
    "axes[2].set_ylabel('-sin(x)', fontsize=12)\n",
    "axes[2].set_title('Second Derivative: -sin(x)', fontsize=14)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ TNN_Sin activation function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc121884",
   "metadata": {},
   "source": [
    "### 3.4 完整的TNN模型\n",
    "\n",
    "现在我们将所有组件组合成一个完整的TNN模型。该模型可以：\n",
    "- 计算基函数值 $\\phi_k^{(i)}(x_k)$\n",
    "- 同时计算一阶导数 $\\frac{\\partial \\phi_k^{(i)}}{\\partial x_k}$\n",
    "- 同时计算二阶导数 $\\frac{\\partial^2 \\phi_k^{(i)}}{\\partial x_k^2}$\n",
    "\n",
    "这对于求解PDE非常重要，因为我们需要计算 $\\nabla u$ 和 $\\Delta u$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d46a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified TNN model for教程demonstration\n",
    "    \n",
    "    Architecture for each dimension:\n",
    "        Input(1) -> Hidden(n1) -> Hidden(n2) -> Output(p)\n",
    "    \n",
    "    Args:\n",
    "        dim: problem dimension\n",
    "        size: [1, n1, n2, p] - network architecture\n",
    "        activation: activation function class\n",
    "        bd, grad_bd, grad_grad_bd: boundary condition functions\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, size, activation, bd=None, grad_bd=None, grad_grad_bd=None):\n",
    "        super(SimpleTNN, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.size = size\n",
    "        self.p = abs(size[-1])  # rank\n",
    "        self.activation = activation()\n",
    "        self.bd = bd\n",
    "        self.grad_bd = grad_bd\n",
    "        self.grad_grad_bd = grad_grad_bd\n",
    "        \n",
    "        # Build network layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(size) - 1):\n",
    "            bias = True if size[i+1] > 0 else False\n",
    "            self.layers.append(TNN_Linear(dim, abs(size[i+1]), abs(size[i]), bias))\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with orthogonal initialization\"\"\"\n",
    "        for layer in self.layers:\n",
    "            for j in range(self.dim):\n",
    "                nn.init.orthogonal_(layer.weight[j, :, :])\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias[j, :, :], 0.5)\n",
    "    \n",
    "    def forward(self, x, need_grad=0):\n",
    "        \"\"\"\n",
    "        Forward pass with optional gradient computation\n",
    "        \n",
    "        Args:\n",
    "            x: input points [N]\n",
    "            need_grad: 0=values only, 1=values+grad, 2=values+grad+grad_grad\n",
    "        \n",
    "        Returns:\n",
    "            phi: basis functions [dim, p, N]\n",
    "            grad_phi (optional): gradients [dim, p, N]\n",
    "            grad_grad_phi (optional): second derivatives [dim, p, N]\n",
    "        \"\"\"\n",
    "        # Expand input for all dimensions\n",
    "        x = x.unsqueeze(0).unsqueeze(0).expand(self.dim, 1, -1)  # [dim, 1, N]\n",
    "        \n",
    "        if need_grad == 0:\n",
    "            # Only compute values\n",
    "            for i, layer in enumerate(self.layers[:-1]):\n",
    "                x = layer(x)\n",
    "                x = self.activation(x)\n",
    "            phi = self.layers[-1](x)\n",
    "            \n",
    "            # Apply boundary condition\n",
    "            if self.bd is not None:\n",
    "                bd_value = self.bd(x[0, 0, :]).unsqueeze(0).unsqueeze(0)  # [1, 1, N]\n",
    "                phi = phi * bd_value\n",
    "            \n",
    "            return phi\n",
    "        \n",
    "        elif need_grad == 1:\n",
    "            # Compute values and first derivatives\n",
    "            grad_x = self.layers[0].weight  # [dim, n1, 1]\n",
    "            \n",
    "            for i in range(len(self.layers) - 1):\n",
    "                x = self.layers[i](x)\n",
    "                grad_x = self.activation.grad(x) * grad_x\n",
    "                if i < len(self.layers) - 2:\n",
    "                    grad_x = self.layers[i+1].weight @ grad_x\n",
    "                x = self.activation(x)\n",
    "            \n",
    "            phi = self.layers[-1](x)\n",
    "            grad_phi = self.layers[-1].weight @ grad_x\n",
    "            \n",
    "            # Apply boundary condition\n",
    "            if self.bd is not None:\n",
    "                x_input = x[0, 0, :]\n",
    "                bd_value = self.bd(x_input).unsqueeze(0).unsqueeze(0)\n",
    "                grad_bd_value = self.grad_bd(x_input).unsqueeze(0).unsqueeze(0)\n",
    "                \n",
    "                grad_phi = phi * grad_bd_value + grad_phi * bd_value\n",
    "                phi = phi * bd_value\n",
    "            \n",
    "            return phi, grad_phi\n",
    "        \n",
    "        else:  # need_grad == 2\n",
    "            # Compute values, first and second derivatives\n",
    "            # (Simplified version - full implementation would be more complex)\n",
    "            phi, grad_phi = self.forward(x[0, 0, :], need_grad=1)\n",
    "            # For tutorial, we'll use automatic differentiation\n",
    "            return phi, grad_phi, None\n",
    "\n",
    "print(\"✓ SimpleTNN model created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de8d69a",
   "metadata": {},
   "source": [
    "## 4. 实例：求解2D Poisson方程\n",
    "\n",
    "### 问题定义\n",
    "\n",
    "我们要求解以下PDE：\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "-\\Delta u = f(x, y) & \\text{in } \\Omega = [-1, 1]^2 \\\\\n",
    "u = 0 & \\text{on } \\partial\\Omega\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "其中右端项为：\n",
    "$$\n",
    "f(x, y) = 2\\pi^2 \\sin(\\pi x) \\sin(\\pi y)\n",
    "$$\n",
    "\n",
    "**精确解**（用于验证）:\n",
    "$$\n",
    "u(x, y) = \\sin(\\pi x) \\sin(\\pi y)\n",
    "$$\n",
    "\n",
    "### 变分形式\n",
    "\n",
    "等价于最小化能量泛函：\n",
    "$$\n",
    "E(u) = \\frac{1}{2}\\int_\\Omega |\\nabla u|^2 dx - \\int_\\Omega f \\cdot u \\, dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbf42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 定义问题参数\n",
    "dim = 2  # 空间维度\n",
    "n_quad = 40  # 每个维度的积分节点数\n",
    "rank = 50  # TNN秩\n",
    "\n",
    "# 定义精确解和右端项\n",
    "def exact_solution(X):\n",
    "    \"\"\"精确解: u(x,y) = sin(πx)sin(πy)\"\"\"\n",
    "    return torch.prod(torch.sin(np.pi * X), dim=1, keepdim=True)\n",
    "\n",
    "def source_term(X):\n",
    "    \"\"\"右端项: f(x,y) = 2π²sin(πx)sin(πy)\"\"\"\n",
    "    return 2 * np.pi**2 * torch.prod(torch.sin(np.pi * X), dim=1, keepdim=True)\n",
    "\n",
    "# 4.2 生成Gauss-Legendre积分节点\n",
    "def gauss_legendre_quadrature(n_quad, dim, a=-1, b=1):\n",
    "    \"\"\"\n",
    "    生成d维Gauss-Legendre积分节点和权重\n",
    "    \n",
    "    参数:\n",
    "        n_quad: 每个维度的节点数\n",
    "        dim: 空间维度\n",
    "        a, b: 积分区间 [a, b]^dim\n",
    "    \n",
    "    返回:\n",
    "        X_quad: 积分节点 [n_quad^dim, dim]\n",
    "        w_quad: 积分权重 [n_quad^dim, 1]\n",
    "    \"\"\"\n",
    "    # 1D Gauss-Legendre节点和权重（区间[-1,1]）\n",
    "    xi_1d, w_1d = np.polynomial.legendre.leggauss(n_quad)\n",
    "    \n",
    "    # 映射到[a, b]\n",
    "    xi_1d = 0.5 * (b - a) * xi_1d + 0.5 * (b + a)\n",
    "    w_1d = 0.5 * (b - a) * w_1d\n",
    "    \n",
    "    # 张量积生成多维节点\n",
    "    xi_list = [xi_1d] * dim\n",
    "    w_list = [w_1d] * dim\n",
    "    \n",
    "    # 使用meshgrid生成所有组合\n",
    "    mesh = np.meshgrid(*xi_list, indexing='ij')\n",
    "    X_quad = np.stack([m.flatten() for m in mesh], axis=1)\n",
    "    \n",
    "    # 权重为各维度权重的乘积\n",
    "    w_mesh = np.meshgrid(*w_list, indexing='ij')\n",
    "    w_quad = np.prod(np.stack([w.flatten() for w in w_mesh], axis=1), axis=1, keepdims=True)\n",
    "    \n",
    "    return torch.tensor(X_quad, dtype=torch.float32), torch.tensor(w_quad, dtype=torch.float32)\n",
    "\n",
    "# 生成积分节点\n",
    "X_quad, w_quad = gauss_legendre_quadrature(n_quad, dim)\n",
    "print(f\"积分节点数量: {X_quad.shape[0]}\")\n",
    "print(f\"节点形状: {X_quad.shape}\")\n",
    "print(f\"权重形状: {w_quad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45af353",
   "metadata": {},
   "source": [
    "### 4.3 定义损失函数\n",
    "\n",
    "根据能量泛函，损失函数包括两部分：\n",
    "1. **梯度能量项**: $\\frac{1}{2}\\int_\\Omega |\\nabla u|^2 dx$\n",
    "2. **源项**: $-\\int_\\Omega f \\cdot u \\, dx$\n",
    "\n",
    "使用Gauss-Legendre积分进行数值计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, X_quad, w_quad):\n",
    "    \"\"\"\n",
    "    计算能量泛函损失\n",
    "    \n",
    "    E(u) = 1/2 ∫|∇u|² dx - ∫f·u dx\n",
    "    \"\"\"\n",
    "    # 前向传播，获取u和∇u\n",
    "    u, grad_u = model(X_quad, need_grad=1)\n",
    "    \n",
    "    # 计算f(X)\n",
    "    f = source_term(X_quad)\n",
    "    \n",
    "    # 梯度能量项: 1/2 ∫|∇u|² dx\n",
    "    grad_energy = 0.5 * torch.sum(w_quad * torch.sum(grad_u**2, dim=1, keepdim=True))\n",
    "    \n",
    "    # 源项: -∫f·u dx\n",
    "    source_energy = -torch.sum(w_quad * f * u)\n",
    "    \n",
    "    # 总损失\n",
    "    loss = grad_energy + source_energy\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 测试损失函数\n",
    "model = SimpleTNN(dim=dim, sizes=[rank]*3, activation=TNN_Sin())\n",
    "loss = compute_loss(model, X_quad, w_quad)\n",
    "print(f\"初始损失: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f923d2e",
   "metadata": {},
   "source": [
    "### 4.4 训练模型\n",
    "\n",
    "使用Adam优化器训练网络，最小化能量泛函。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45173d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新初始化模型\n",
    "model = SimpleTNN(dim=dim, sizes=[rank]*3, activation=TNN_Sin())\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练参数\n",
    "n_epochs = 5000\n",
    "print_every = 500\n",
    "\n",
    "# 记录训练历史\n",
    "loss_history = []\n",
    "\n",
    "print(\"开始训练...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # 梯度清零\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = compute_loss(model, X_quad, w_quad)\n",
    "    \n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 记录损失\n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    # 打印进度\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f\"Epoch {epoch+1:5d}/{n_epochs} | Loss: {loss.item():.8f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cdb34",
   "metadata": {},
   "source": [
    "## 5. 结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1165c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 损失曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(loss_history)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (Energy)', fontsize=12)\n",
    "plt.title('训练损失曲线', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"最终损失: {loss_history[-1]:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc19465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 解的对比图\n",
    "# 生成测试网格\n",
    "n_test = 50\n",
    "x_test = np.linspace(-1, 1, n_test)\n",
    "y_test = np.linspace(-1, 1, n_test)\n",
    "X_mesh, Y_mesh = np.meshgrid(x_test, y_test)\n",
    "\n",
    "# 准备输入数据\n",
    "X_test = torch.tensor(np.stack([X_mesh.flatten(), Y_mesh.flatten()], axis=1), dtype=torch.float32)\n",
    "\n",
    "# 模型预测\n",
    "with torch.no_grad():\n",
    "    u_pred = model(X_test, need_grad=0).numpy().reshape(n_test, n_test)\n",
    "\n",
    "# 精确解\n",
    "u_exact = exact_solution(X_test).numpy().reshape(n_test, n_test)\n",
    "\n",
    "# 误差\n",
    "error = np.abs(u_pred - u_exact)\n",
    "\n",
    "# 绘图\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 精确解\n",
    "im0 = axes[0, 0].contourf(X_mesh, Y_mesh, u_exact, levels=20, cmap='RdBu_r')\n",
    "axes[0, 0].set_title('精确解 $u_{exact}$', fontsize=14)\n",
    "axes[0, 0].set_xlabel('$x$', fontsize=12)\n",
    "axes[0, 0].set_ylabel('$y$', fontsize=12)\n",
    "plt.colorbar(im0, ax=axes[0, 0])\n",
    "\n",
    "# TNN预测解\n",
    "im1 = axes[0, 1].contourf(X_mesh, Y_mesh, u_pred, levels=20, cmap='RdBu_r')\n",
    "axes[0, 1].set_title('TNN预测解 $u_{TNN}$', fontsize=14)\n",
    "axes[0, 1].set_xlabel('$x$', fontsize=12)\n",
    "axes[0, 1].set_ylabel('$y$', fontsize=12)\n",
    "plt.colorbar(im1, ax=axes[0, 1])\n",
    "\n",
    "# 误差分布\n",
    "im2 = axes[1, 0].contourf(X_mesh, Y_mesh, error, levels=20, cmap='hot')\n",
    "axes[1, 0].set_title('绝对误差 $|u_{TNN} - u_{exact}|$', fontsize=14)\n",
    "axes[1, 0].set_xlabel('$x$', fontsize=12)\n",
    "axes[1, 0].set_ylabel('$y$', fontsize=12)\n",
    "plt.colorbar(im2, ax=axes[1, 0])\n",
    "\n",
    "# 中心线对比\n",
    "center_idx = n_test // 2\n",
    "axes[1, 1].plot(x_test, u_exact[center_idx, :], 'b-', linewidth=2, label='精确解')\n",
    "axes[1, 1].plot(x_test, u_pred[center_idx, :], 'r--', linewidth=2, label='TNN预测')\n",
    "axes[1, 1].set_title('中心线 ($y=0$) 对比', fontsize=14)\n",
    "axes[1, 1].set_xlabel('$x$', fontsize=12)\n",
    "axes[1, 1].set_ylabel('$u$', fontsize=12)\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 计算误差指标\n",
    "l2_error = np.sqrt(np.mean(error**2))\n",
    "max_error = np.max(error)\n",
    "print(f\"\\n误差统计:\")\n",
    "print(f\"  L2 误差: {l2_error:.6e}\")\n",
    "print(f\"  最大误差: {max_error:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a456733",
   "metadata": {},
   "source": [
    "## 6. 总结与拓展\n",
    "\n",
    "### TNN的优势\n",
    "\n",
    "1. **指数级复杂度降低**: 对于d维问题，参数量从$O(N^d)$降至$O(dRN^2)$\n",
    "2. **自动满足边界条件**: 通过构造方式强制满足\n",
    "3. **高精度**: 使用高阶积分和自动微分\n",
    "4. **易于扩展**: 可应用于更复杂的PDE系统\n",
    "\n",
    "### 进一步探索\n",
    "\n",
    "- **非齐次边界条件**: 使用两个TNN的分解方法（参考ex_5_2）\n",
    "- **Neumann边界**: 采用弱形式（参考ex_5_3）\n",
    "- **特征值问题**: Rayleigh商优化（参考ex_5_4）\n",
    "- **无界区域**: Hermite-Gauss积分（参考ex_5_5）\n",
    "- **更高维度**: 扩展到5D、10D甚至20D问题\n",
    "\n",
    "### 参考文献\n",
    "\n",
    "本教程基于以下论文的实现：\n",
    "- TNN for high-dimensional PDEs\n",
    "- Tensor decomposition methods for scientific computing\n",
    "\n",
    "完整代码见：`TNN/example/` 目录"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
