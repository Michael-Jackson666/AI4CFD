# æ¨¡å‹æ¶æ„é€‰æ‹©æŒ‡å—
# Model Architecture Selection Guide

## ğŸ¯ å¿«é€Ÿå¼€å§‹

åœ¨ `main.py` ä¸­ï¼Œåªéœ€è¦ä¿®æ”¹ä¸€è¡Œä»£ç å³å¯åˆ‡æ¢æ¨¡å‹ï¼š

```python
'model_type': 'mlp',  # æ”¹æˆä½ æƒ³è¦çš„æ¨¡å‹ç±»å‹
```

## ğŸ“¦ å¯ç”¨çš„æ¨¡å‹æ¶æ„

### 1. **MLP** (Multi-Layer Perceptron)
ä¼ ç»Ÿçš„å¤šå±‚æ„ŸçŸ¥æœºï¼ŒPINN çš„ç»å…¸æ¶æ„ã€‚

**ä½¿ç”¨æ–¹å¼ï¼š**
```python
configuration = {
    'model_type': 'mlp',
    'nn_layers': 8,      # éšè—å±‚æ•°
    'nn_neurons': 128,   # æ¯å±‚ç¥ç»å…ƒæ•°
    # ... å…¶ä»–é…ç½®
}
```

**ç‰¹ç‚¹ï¼š**
- âœ… è®­ç»ƒé€Ÿåº¦å¿«
- âœ… ç¨³å®šå¯é 
- âœ… å‚æ•°é‡è¾ƒå°
- âœ… é€‚åˆå¤§å¤šæ•°é—®é¢˜

**é€‚ç”¨åœºæ™¯ï¼š**
- å¿«é€ŸåŸå‹å¼€å‘
- æ ‡å‡† PINN é—®é¢˜
- è®¡ç®—èµ„æºæœ‰é™

**å‚æ•°é‡ç¤ºä¾‹ï¼š**
- 8 å±‚ Ã— 128 ç¥ç»å…ƒ â‰ˆ 133K å‚æ•°
- 12 å±‚ Ã— 256 ç¥ç»å…ƒ â‰ˆ 788K å‚æ•°

---

### 2. **Transformer**
åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ Transformer æ¶æ„ã€‚

**ä½¿ç”¨æ–¹å¼ï¼š**
```python
configuration = {
    'model_type': 'transformer',
    'd_model': 256,                    # åµŒå…¥ç»´åº¦
    'nhead': 8,                        # æ³¨æ„åŠ›å¤´æ•°
    'num_transformer_layers': 6,       # Transformer å±‚æ•°
    'dim_feedforward': 1024,           # å‰é¦ˆç½‘ç»œç»´åº¦
    'dropout': 0.1,                    # Dropout ç‡
    # ... å…¶ä»–é…ç½®
}
```

**ç‰¹ç‚¹ï¼š**
- âœ… èƒ½æ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»
- âœ… è‡ªæ³¨æ„åŠ›æœºåˆ¶
- âœ… é€‚åˆå¤æ‚æ¨¡å¼
- âš ï¸ å‚æ•°é‡å¤§
- âš ï¸ è®­ç»ƒè¾ƒæ…¢

**é€‚ç”¨åœºæ™¯ï¼š**
- å¤æ‚çš„ç‰©ç†ç°è±¡
- éœ€è¦æ•æ‰å…¨å±€ç›¸å…³æ€§
- æœ‰è¶³å¤Ÿçš„è®¡ç®—èµ„æº

**å‚æ•°é‡ç¤ºä¾‹ï¼š**
- æ ‡å‡†é…ç½® (d=256, h=8, l=6) â‰ˆ 2.5M å‚æ•°

---

### 3. **Lightweight Transformer**
è½»é‡çº§ Transformerï¼Œå‚æ•°é‡æ›´å°‘ï¼Œè®­ç»ƒæ›´å¿«ã€‚

**ä½¿ç”¨æ–¹å¼ï¼š**
```python
configuration = {
    'model_type': 'lightweight_transformer',
    'd_model': 128,                    # è¾ƒå°çš„åµŒå…¥ç»´åº¦
    'nhead': 4,                        # è¾ƒå°‘çš„æ³¨æ„åŠ›å¤´
    'num_transformer_layers': 3,       # è¾ƒå°‘çš„å±‚æ•°
    'dim_feedforward': 512,            # è¾ƒå°çš„å‰é¦ˆç»´åº¦
    'dropout': 0.1,
    # ... å…¶ä»–é…ç½®
}
```

**ç‰¹ç‚¹ï¼š**
- âœ… Transformer çš„ä¼˜ç‚¹
- âœ… è®­ç»ƒé€Ÿåº¦æ›´å¿«
- âœ… å‚æ•°é‡é€‚ä¸­
- âš ï¸ è¡¨è¾¾èƒ½åŠ›ç•¥å¼±äºæ ‡å‡† Transformer

**é€‚ç”¨åœºæ™¯ï¼š**
- æƒ³å°è¯• Transformer ä½†èµ„æºæœ‰é™
- å¿«é€Ÿå®éªŒ
- ä¸­ç­‰å¤æ‚åº¦é—®é¢˜

**å‚æ•°é‡ç¤ºä¾‹ï¼š**
- è½»é‡é…ç½® (d=128, h=4, l=3) â‰ˆ 600K å‚æ•°

---

### 4. **Hybrid Transformer**
æ··åˆæ¶æ„ï¼Œç»“åˆ Transformer å’Œ MLP çš„ä¼˜åŠ¿ã€‚

**ä½¿ç”¨æ–¹å¼ï¼š**
```python
configuration = {
    'model_type': 'hybrid_transformer',
    'd_model': 256,                    # Transformer åµŒå…¥ç»´åº¦
    'nhead': 8,                        # æ³¨æ„åŠ›å¤´æ•°
    'num_transformer_layers': 4,       # Transformer å±‚æ•°
    'num_mlp_layers': 4,               # MLP åˆ†æ”¯å±‚æ•°
    'mlp_neurons': 512,                # MLP æ¯å±‚ç¥ç»å…ƒæ•°
    'dropout': 0.1,
    # ... å…¶ä»–é…ç½®
}
```

**ç‰¹ç‚¹ï¼š**
- âœ… ç»“åˆä¸¤ç§æ¶æ„çš„ä¼˜åŠ¿
- âœ… Transformer æ•æ‰å…¨å±€ç‰¹å¾
- âœ… MLP æ•æ‰å±€éƒ¨ç‰¹å¾
- âš ï¸ å‚æ•°é‡æœ€å¤§
- âš ï¸ è®­ç»ƒæœ€æ…¢

**é€‚ç”¨åœºæ™¯ï¼š**
- æœ€å¤æ‚çš„é—®é¢˜
- éœ€è¦åŒæ—¶æ•æ‰å…¨å±€å’Œå±€éƒ¨ç‰¹å¾
- è¿½æ±‚æœ€é«˜ç²¾åº¦
- æœ‰å……è¶³çš„è®¡ç®—èµ„æº

**å‚æ•°é‡ç¤ºä¾‹ï¼š**
- æ··åˆé…ç½® â‰ˆ 3M+ å‚æ•°

---

## ğŸ“Š æ¨¡å‹å¯¹æ¯”è¡¨

| æ¨¡å‹ç±»å‹ | å‚æ•°é‡ | è®­ç»ƒé€Ÿåº¦ | è¡¨è¾¾èƒ½åŠ› | å†…å­˜å ç”¨ | æ¨èåœºæ™¯ |
|---------|-------|---------|---------|---------|---------|
| MLP | â­ å° | â­â­â­ å¿« | â­â­ ä¸­ | â­ ä½ | å¿«é€ŸåŸå‹ã€æ ‡å‡†é—®é¢˜ |
| Lightweight Transformer | â­â­ ä¸­ | â­â­ ä¸­ | â­â­â­ å¼º | â­â­ ä¸­ | å®éªŒã€ä¸­ç­‰å¤æ‚åº¦ |
| Transformer | â­â­â­ å¤§ | â­ æ…¢ | â­â­â­â­ å¾ˆå¼º | â­â­â­ é«˜ | å¤æ‚é—®é¢˜ã€å…¨å±€ç›¸å…³æ€§ |
| Hybrid Transformer | â­â­â­â­ å¾ˆå¤§ | â­ å¾ˆæ…¢ | â­â­â­â­â­ æœ€å¼º | â­â­â­â­ å¾ˆé«˜ | æœ€å¤æ‚é—®é¢˜ã€é«˜ç²¾åº¦ |

---

## ğŸ’¡ ä½¿ç”¨å»ºè®®

### ç¬¬ä¸€æ¬¡ä½¿ç”¨
```python
'model_type': 'mlp',
'nn_layers': 8,
'nn_neurons': 128,
'epochs': 500,  # å¿«é€Ÿæµ‹è¯•
```

### æ ‡å‡†è®­ç»ƒ
```python
'model_type': 'mlp',
'nn_layers': 12,
'nn_neurons': 256,
'epochs': 2000,
```

### å°è¯• Transformer
```python
'model_type': 'lightweight_transformer',
'd_model': 128,
'nhead': 4,
'num_transformer_layers': 3,
'epochs': 1000,
```

### é«˜ç²¾åº¦è®­ç»ƒ
```python
'model_type': 'transformer',  # æˆ– 'hybrid_transformer'
'd_model': 256,
'nhead': 8,
'num_transformer_layers': 6,
'epochs': 5000,
'learning_rate': 5e-5,  # é™ä½å­¦ä¹ ç‡
```

---

## ğŸ”§ å¸¸è§é…ç½®ç»„åˆ

### é…ç½® 1: å¿«é€Ÿæµ‹è¯• (5-10 åˆ†é’Ÿ)
```python
configuration = {
    'model_type': 'mlp',
    'nn_layers': 6,
    'nn_neurons': 64,
    'epochs': 500,
    'plot_dir': 'quick_test'
}
```

### é…ç½® 2: æ ‡å‡† MLP (30 åˆ†é’Ÿ)
```python
configuration = {
    'model_type': 'mlp',
    'nn_layers': 8,
    'nn_neurons': 128,
    'epochs': 2000,
    'plot_dir': 'mlp_standard'
}
```

### é…ç½® 3: è½»é‡ Transformer (45 åˆ†é’Ÿ)
```python
configuration = {
    'model_type': 'lightweight_transformer',
    'd_model': 128,
    'nhead': 4,
    'num_transformer_layers': 3,
    'epochs': 1500,
    'plot_dir': 'transformer_light'
}
```

### é…ç½® 4: æ ‡å‡† Transformer (2 å°æ—¶)
```python
configuration = {
    'model_type': 'transformer',
    'd_model': 256,
    'nhead': 8,
    'num_transformer_layers': 6,
    'epochs': 2000,
    'learning_rate': 5e-5,
    'plot_dir': 'transformer_standard'
}
```

### é…ç½® 5: æ··åˆæ¨¡å‹ - é«˜ç²¾åº¦ (4+ å°æ—¶)
```python
configuration = {
    'model_type': 'hybrid_transformer',
    'd_model': 256,
    'nhead': 8,
    'num_transformer_layers': 4,
    'num_mlp_layers': 4,
    'mlp_neurons': 512,
    'epochs': 5000,
    'learning_rate': 5e-5,
    'plot_dir': 'hybrid_high_precision'
}
```

---

## ğŸ“ˆ æ€§èƒ½è°ƒä¼˜å»ºè®®

### å¦‚æœè®­ç»ƒä¸ç¨³å®šï¼š
1. é™ä½å­¦ä¹ ç‡ï¼š`'learning_rate': 5e-5` æˆ– `1e-5`
2. å¢åŠ  dropoutï¼š`'dropout': 0.2`
3. ä½¿ç”¨æ¢¯åº¦è£å‰ªï¼ˆå·²å†…ç½®ï¼‰

### å¦‚æœè¿‡æ‹Ÿåˆï¼š
1. å¢åŠ  dropoutï¼š`'dropout': 0.2` æˆ– `0.3`
2. å‡å°‘æ¨¡å‹å¤æ‚åº¦ï¼ˆå‡å°‘å±‚æ•°æˆ–ç¥ç»å…ƒæ•°ï¼‰
3. å¢åŠ æ­£åˆ™åŒ–æƒé‡

### å¦‚æœæ¬ æ‹Ÿåˆï¼š
1. å¢åŠ æ¨¡å‹å®¹é‡ï¼ˆæ›´å¤šå±‚æˆ–ç¥ç»å…ƒï¼‰
2. å¢åŠ è®­ç»ƒè½®æ•°
3. é™ä½å­¦ä¹ ç‡ï¼Œè®­ç»ƒæ›´ä¹…

---

## ğŸš€ å¿«é€Ÿåˆ‡æ¢ç¤ºä¾‹

**åœ¨ `main.py` ä¸­ï¼Œæ‰¾åˆ°é…ç½®éƒ¨åˆ†ï¼Œå–æ¶ˆæ³¨é‡Šç›¸åº”çš„é¢„è®¾ï¼š**

```python
# ============================================================
# QUICK CONFIGURATION PRESETS (uncomment to use)
# ============================================================

# Preset 1: Standard MLP (default, fast training)
configuration['model_type'] = 'mlp'
configuration['nn_layers'] = 8
configuration['nn_neurons'] = 128

# # Preset 3: Standard Transformer (good for complex patterns)
# configuration['model_type'] = 'transformer'
# configuration['d_model'] = 256
# configuration['nhead'] = 8
# configuration['num_transformer_layers'] = 6

# # Preset 5: Hybrid Model (combines both approaches)
# configuration['model_type'] = 'hybrid_transformer'
# configuration['d_model'] = 256
# configuration['nhead'] = 8
# configuration['num_transformer_layers'] = 4
# configuration['num_mlp_layers'] = 4
```

åªéœ€è¦æ³¨é‡Šæ‰å½“å‰çš„ï¼Œå–æ¶ˆæ³¨é‡Šä½ æƒ³ç”¨çš„å³å¯ï¼

---

## ğŸ“ è¿è¡Œå‘½ä»¤

```bash
cd /Users/jack/Desktop/ML/AI4CFD/PINNs/vp_system
python main.py
```

æ¨¡å‹ä¼šè‡ªåŠ¨æ ¹æ® `model_type` é€‰æ‹©å¯¹åº”çš„æ¶æ„å¹¶å¼€å§‹è®­ç»ƒï¼

---

**æç¤º**: å»ºè®®å…ˆç”¨ MLP å¿«é€Ÿæµ‹è¯•ï¼Œç¡®ä¿ä»£ç æ­£å¸¸è¿è¡Œï¼Œç„¶åå†å°è¯• Transformer æ¶æ„ï¼
