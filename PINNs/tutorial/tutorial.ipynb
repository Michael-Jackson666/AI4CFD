{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-Informed Neural Networks (PINNs) Tutorial\n",
    "\n",
    "This notebook provides a comprehensive introduction to Physics-Informed Neural Networks (PINNs) for solving partial differential equations (PDEs). We'll implement and train PINNs to solve various types of PDEs.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to PINNs](#introduction)\n",
    "2. [Mathematical Background](#mathematical-background)\n",
    "3. [Implementation](#implementation)\n",
    "4. [Example 1: 1D Poisson Equation](#example-1)\n",
    "5. [Example 2: 2D Poisson Equation](#example-2)\n",
    "6. [Example 3: Burgers' Equation](#example-3)\n",
    "7. [Advanced Topics](#advanced-topics)\n",
    "8. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to PINNs {#introduction}\n",
    "\n",
    "Physics-Informed Neural Networks (PINNs) are a revolutionary approach to solving partial differential equations (PDEs) using deep learning. Unlike traditional numerical methods, PINNs:\n",
    "\n",
    "- **Incorporate physics directly**: PDEs are embedded as regularization terms in the loss function\n",
    "- **Handle complex geometries**: No need for mesh generation\n",
    "- **Work with limited data**: Can solve PDEs with minimal or no training data\n",
    "- **Provide smooth solutions**: Neural networks naturally produce smooth, differentiable solutions\n",
    "\n",
    "### Key Advantages:\n",
    "- Mesh-free approach\n",
    "- Automatic differentiation for computing derivatives\n",
    "- Integration of multiple physics constraints\n",
    "- Uncertainty quantification capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematical Background {#mathematical-background}\n",
    "\n",
    "### General PDE Form\n",
    "Consider a general PDE of the form:\n",
    "$$F(\\mathbf{x}, u, \\nabla u, \\nabla^2 u, \\ldots) = 0$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{x}$ represents the input coordinates (spatial and/or temporal)\n",
    "- $u$ is the solution we want to find\n",
    "- $\\nabla u$, $\\nabla^2 u$ are the derivatives of $u$\n",
    "\n",
    "### PINN Approach\n",
    "1. **Neural Network Approximation**: Use a neural network $u_{\\theta}(\\mathbf{x})$ to approximate the solution\n",
    "2. **Physics Loss**: Define the physics loss as:\n",
    "   $$\\mathcal{L}_{physics} = \\frac{1}{N_{collocation}} \\sum_{i=1}^{N_{collocation}} |F(\\mathbf{x}_i, u_{\\theta}(\\mathbf{x}_i), \\nabla u_{\\theta}(\\mathbf{x}_i), \\ldots)|^2$$\n",
    "\n",
    "3. **Boundary/Initial Conditions**: Add losses for boundary and initial conditions:\n",
    "   $$\\mathcal{L}_{BC} = \\frac{1}{N_{BC}} \\sum_{i=1}^{N_{BC}} |u_{\\theta}(\\mathbf{x}_{BC,i}) - u_{BC,i}|^2$$\n",
    "\n",
    "4. **Total Loss**: Combine all losses:\n",
    "   $$\\mathcal{L}_{total} = \\lambda_{physics} \\mathcal{L}_{physics} + \\lambda_{BC} \\mathcal{L}_{BC} + \\lambda_{data} \\mathcal{L}_{data}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation {#implementation}\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic PINN Architecture\n",
    "\n",
    "Let's implement a simple PINN architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePINN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Physics-Informed Neural Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1, hidden_dim=50, output_dim=1, num_layers=4):\n",
    "        super(SimplePINN, self).__init__()\n",
    "        \n",
    "        # Build the network\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.Tanh())\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.Tanh())\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize network weights using Xavier initialization.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Test the network\n",
    "model = SimplePINN(input_dim=1, hidden_dim=32, output_dim=1, num_layers=3)\n",
    "test_input = torch.randn(10, 1)\n",
    "test_output = model(test_input)\n",
    "print(f\"Model input shape: {test_input.shape}\")\n",
    "print(f\"Model output shape: {test_output.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation Helper Function\n",
    "\n",
    "PINNs rely heavily on automatic differentiation to compute derivatives of the neural network output with respect to its inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivatives(u, x, order=1):\n",
    "    \"\"\"\n",
    "    Compute derivatives of u with respect to x using automatic differentiation.\n",
    "    \n",
    "    Args:\n",
    "        u: Neural network output\n",
    "        x: Input coordinates (requires_grad=True)\n",
    "        order: Order of derivative (1 or 2)\n",
    "    \n",
    "    Returns:\n",
    "        List of derivatives with respect to each coordinate\n",
    "    \"\"\"\n",
    "    grad_outputs = torch.ones_like(u)\n",
    "    derivatives = []\n",
    "    \n",
    "    for i in range(x.shape[1]):\n",
    "        if order == 1:\n",
    "            # First derivative\n",
    "            grad_u = torch.autograd.grad(u, x, grad_outputs=grad_outputs,\n",
    "                                       create_graph=True, retain_graph=True)[0]\n",
    "            derivative = grad_u[:, i:i+1]\n",
    "        elif order == 2:\n",
    "            # First derivative\n",
    "            grad_u = torch.autograd.grad(u, x, grad_outputs=grad_outputs,\n",
    "                                       create_graph=True, retain_graph=True)[0]\n",
    "            u_x = grad_u[:, i:i+1]\n",
    "            # Second derivative\n",
    "            derivative = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x),\n",
    "                                           create_graph=True, retain_graph=True)[0][:, i:i+1]\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Order {order} derivatives not implemented\")\n",
    "        \n",
    "        derivatives.append(derivative)\n",
    "    \n",
    "    return derivatives\n",
    "\n",
    "# Test automatic differentiation\n",
    "x = torch.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "x.requires_grad_(True)\n",
    "\n",
    "# Test function: u = sin(Ï€x)\n",
    "u_test = torch.sin(torch.pi * x)\n",
    "\n",
    "# Compute derivatives\n",
    "u_x = compute_derivatives(u_test, x, order=1)[0]\n",
    "u_xx = compute_derivatives(u_test, x, order=2)[0]\n",
    "\n",
    "# Analytical derivatives for comparison\n",
    "u_x_analytical = torch.pi * torch.cos(torch.pi * x)\n",
    "u_xx_analytical = -torch.pi**2 * torch.sin(torch.pi * x)\n",
    "\n",
    "print(f\"First derivative error: {torch.mean((u_x - u_x_analytical)**2).item():.2e}\")\n",
    "print(f\"Second derivative error: {torch.mean((u_xx - u_xx_analytical)**2).item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example 1: 1D Poisson Equation {#example-1}\n",
    "\n",
    "Let's solve our first PDE: the 1D Poisson equation.\n",
    "\n",
    "### Problem Setup\n",
    "$$\\frac{d^2 u}{dx^2} = -\\pi^2 \\sin(\\pi x), \\quad x \\in [-1, 1]$$\n",
    "$$u(-1) = u(1) = 0 \\quad \\text{(boundary conditions)}$$\n",
    "\n",
    "**Analytical Solution**: $u(x) = \\sin(\\pi x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poisson1D_PINN:\n",
    "    \"\"\"\n",
    "    PINN for solving 1D Poisson equation.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        self.history = {'loss': [], 'pde_loss': [], 'bc_loss': []}\n",
    "    \n",
    "    def pde_loss(self, x_collocation):\n",
    "        \"\"\"Compute PDE residual loss.\"\"\"\n",
    "        x_collocation.requires_grad_(True)\n",
    "        u = self.model(x_collocation)\n",
    "        \n",
    "        # Compute second derivative\n",
    "        u_xx = compute_derivatives(u, x_collocation, order=2)[0]\n",
    "        \n",
    "        # Source term: f(x) = -Ï€Â²sin(Ï€x)\n",
    "        f = -torch.pi**2 * torch.sin(torch.pi * x_collocation)\n",
    "        \n",
    "        # PDE residual: dÂ²u/dxÂ² - f = 0\n",
    "        residual = u_xx - f\n",
    "        return torch.mean(residual**2)\n",
    "    \n",
    "    def boundary_loss(self, x_boundary, u_boundary):\n",
    "        \"\"\"Compute boundary condition loss.\"\"\"\n",
    "        u_pred = self.model(x_boundary)\n",
    "        return torch.mean((u_pred - u_boundary)**2)\n",
    "    \n",
    "    def train(self, x_collocation, x_boundary, u_boundary, epochs=5000, \n",
    "              lambda_pde=1.0, lambda_bc=100.0, print_every=1000):\n",
    "        \"\"\"Train the PINN.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Compute losses\n",
    "            pde_loss = self.pde_loss(x_collocation)\n",
    "            bc_loss = self.boundary_loss(x_boundary, u_boundary)\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = lambda_pde * pde_loss + lambda_bc * bc_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Record history\n",
    "            self.history['loss'].append(total_loss.item())\n",
    "            self.history['pde_loss'].append(pde_loss.item())\n",
    "            self.history['bc_loss'].append(bc_loss.item())\n",
    "            \n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                print(f\"Epoch {epoch+1:5d}: Total = {total_loss.item():.6e}, \"\n",
    "                      f\"PDE = {pde_loss.item():.6e}, BC = {bc_loss.item():.6e}\")\n",
    "\n",
    "# Create training data\n",
    "n_collocation = 1000\n",
    "x_collocation = torch.rand(n_collocation, 1) * 2 - 1  # Random points in [-1, 1]\n",
    "\n",
    "# Boundary conditions\n",
    "x_boundary = torch.tensor([[-1.0], [1.0]], dtype=torch.float32)\n",
    "u_boundary = torch.zeros_like(x_boundary)\n",
    "\n",
    "# Create model and trainer\n",
    "model_1d = SimplePINN(input_dim=1, hidden_dim=32, output_dim=1, num_layers=4)\n",
    "trainer_1d = Poisson1D_PINN(model_1d)\n",
    "\n",
    "print(\"Training 1D Poisson PINN...\")\n",
    "trainer_1d.train(x_collocation, x_boundary, u_boundary, epochs=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and visualize results\n",
    "x_test = torch.linspace(-1, 1, 200).reshape(-1, 1)\n",
    "with torch.no_grad():\n",
    "    u_pred = model_1d(x_test)\n",
    "    u_exact = torch.sin(torch.pi * x_test)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Solution comparison\n",
    "axes[0].plot(x_test.numpy(), u_pred.numpy(), 'b-', linewidth=2, label='PINN')\n",
    "axes[0].plot(x_test.numpy(), u_exact.numpy(), 'r--', linewidth=2, label='Exact')\n",
    "axes[0].scatter(x_boundary.numpy(), u_boundary.numpy(), c='red', s=50, zorder=5, label='BC')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('u(x)')\n",
    "axes[0].set_title('1D Poisson Solution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error plot\n",
    "error = torch.abs(u_pred - u_exact)\n",
    "axes[1].plot(x_test.numpy(), error.numpy(), 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('|Error|')\n",
    "axes[1].set_title(f'Absolute Error (Max: {error.max().item():.2e})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training history\n",
    "axes[2].semilogy(trainer_1d.history['loss'], label='Total Loss')\n",
    "axes[2].semilogy(trainer_1d.history['pde_loss'], label='PDE Loss')\n",
    "axes[2].semilogy(trainer_1d.history['bc_loss'], label='BC Loss')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].set_title('Training History')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute errors\n",
    "mse = torch.mean((u_pred - u_exact)**2).item()\n",
    "rel_l2 = torch.norm(u_pred - u_exact) / torch.norm(u_exact)\n",
    "print(f\"Mean Squared Error: {mse:.2e}\")\n",
    "print(f\"Relative L2 Error: {rel_l2.item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example 2: 2D Poisson Equation {#example-2}\n",
    "\n",
    "Now let's tackle a 2D problem:\n",
    "\n",
    "### Problem Setup\n",
    "$$\\nabla^2 u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = -2\\pi^2 \\sin(\\pi x) \\sin(\\pi y)$$\n",
    "$$u(x,y) = 0 \\text{ on boundary of } [-1,1] \\times [-1,1]$$\n",
    "\n",
    "**Analytical Solution**: $u(x,y) = \\sin(\\pi x) \\sin(\\pi y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poisson2D_PINN:\n",
    "    \"\"\"\n",
    "    PINN for solving 2D Poisson equation.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        self.history = {'loss': [], 'pde_loss': [], 'bc_loss': []}\n",
    "    \n",
    "    def pde_loss(self, xy_collocation):\n",
    "        \"\"\"Compute PDE residual loss.\"\"\"\n",
    "        xy_collocation.requires_grad_(True)\n",
    "        u = self.model(xy_collocation)\n",
    "        \n",
    "        # Compute second derivatives\n",
    "        u_xx = compute_derivatives(u, xy_collocation, order=2)[0]  # âˆ‚Â²u/âˆ‚xÂ²\n",
    "        u_yy = compute_derivatives(u, xy_collocation, order=2)[1]  # âˆ‚Â²u/âˆ‚yÂ²\n",
    "        \n",
    "        # Laplacian\n",
    "        laplacian = u_xx + u_yy\n",
    "        \n",
    "        # Source term\n",
    "        x, y = xy_collocation[:, 0:1], xy_collocation[:, 1:2]\n",
    "        f = -2 * torch.pi**2 * torch.sin(torch.pi * x) * torch.sin(torch.pi * y)\n",
    "        \n",
    "        # PDE residual\n",
    "        residual = laplacian - f\n",
    "        return torch.mean(residual**2)\n",
    "    \n",
    "    def boundary_loss(self, xy_boundary):\n",
    "        \"\"\"Compute boundary condition loss (homogeneous Dirichlet).\"\"\"\n",
    "        u_pred = self.model(xy_boundary)\n",
    "        return torch.mean(u_pred**2)\n",
    "    \n",
    "    def train(self, xy_collocation, xy_boundary, epochs=5000, \n",
    "              lambda_pde=1.0, lambda_bc=100.0, print_every=1000):\n",
    "        \"\"\"Train the PINN.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Compute losses\n",
    "            pde_loss = self.pde_loss(xy_collocation)\n",
    "            bc_loss = self.boundary_loss(xy_boundary)\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = lambda_pde * pde_loss + lambda_bc * bc_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Record history\n",
    "            self.history['loss'].append(total_loss.item())\n",
    "            self.history['pde_loss'].append(pde_loss.item())\n",
    "            self.history['bc_loss'].append(bc_loss.item())\n",
    "            \n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                print(f\"Epoch {epoch+1:5d}: Total = {total_loss.item():.6e}, \"\n",
    "                      f\"PDE = {pde_loss.item():.6e}, BC = {bc_loss.item():.6e}\")\n",
    "\n",
    "# Create 2D training data\n",
    "n_collocation = 2000\n",
    "xy_collocation = torch.rand(n_collocation, 2) * 2 - 1  # Random points in [-1,1]Â²\n",
    "\n",
    "# Boundary points\n",
    "n_boundary = 400\n",
    "# Left and right boundaries\n",
    "xy_left = torch.cat([torch.full((n_boundary//4, 1), -1), \n",
    "                     torch.rand(n_boundary//4, 1) * 2 - 1], dim=1)\n",
    "xy_right = torch.cat([torch.full((n_boundary//4, 1), 1), \n",
    "                      torch.rand(n_boundary//4, 1) * 2 - 1], dim=1)\n",
    "# Bottom and top boundaries\n",
    "xy_bottom = torch.cat([torch.rand(n_boundary//4, 1) * 2 - 1, \n",
    "                       torch.full((n_boundary//4, 1), -1)], dim=1)\n",
    "xy_top = torch.cat([torch.rand(n_boundary//4, 1) * 2 - 1, \n",
    "                    torch.full((n_boundary//4, 1), 1)], dim=1)\n",
    "\n",
    "xy_boundary = torch.cat([xy_left, xy_right, xy_bottom, xy_top], dim=0)\n",
    "\n",
    "# Create model and trainer\n",
    "model_2d = SimplePINN(input_dim=2, hidden_dim=64, output_dim=1, num_layers=5)\n",
    "trainer_2d = Poisson2D_PINN(model_2d)\n",
    "\n",
    "print(\"Training 2D Poisson PINN...\")\n",
    "trainer_2d.train(xy_collocation, xy_boundary, epochs=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and visualize 2D results\n",
    "n_test = 50\n",
    "x_test = torch.linspace(-1, 1, n_test)\n",
    "y_test = torch.linspace(-1, 1, n_test)\n",
    "X_test, Y_test = torch.meshgrid(x_test, y_test, indexing='ij')\n",
    "xy_test = torch.stack([X_test.flatten(), Y_test.flatten()], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_pred = model_2d(xy_test).reshape(n_test, n_test)\n",
    "    u_exact = torch.sin(torch.pi * X_test) * torch.sin(torch.pi * Y_test)\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "X_np = X_test.numpy()\n",
    "Y_np = Y_test.numpy()\n",
    "u_pred_np = u_pred.numpy()\n",
    "u_exact_np = u_exact.numpy()\n",
    "error_np = np.abs(u_pred_np - u_exact_np)\n",
    "\n",
    "# Create 2D plots\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Predicted solution\n",
    "ax1 = fig.add_subplot(2, 3, 1, projection='3d')\n",
    "surf1 = ax1.plot_surface(X_np, Y_np, u_pred_np, cmap='viridis', alpha=0.8)\n",
    "ax1.set_title('PINN Solution')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('u')\n",
    "\n",
    "# Exact solution\n",
    "ax2 = fig.add_subplot(2, 3, 2, projection='3d')\n",
    "surf2 = ax2.plot_surface(X_np, Y_np, u_exact_np, cmap='viridis', alpha=0.8)\n",
    "ax2.set_title('Exact Solution')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_zlabel('u')\n",
    "\n",
    "# Error\n",
    "ax3 = fig.add_subplot(2, 3, 3, projection='3d')\n",
    "surf3 = ax3.plot_surface(X_np, Y_np, error_np, cmap='Reds', alpha=0.8)\n",
    "ax3.set_title('Absolute Error')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_zlabel('|Error|')\n",
    "\n",
    "# Contour plots\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "contour1 = ax4.contourf(X_np, Y_np, u_pred_np, levels=20, cmap='viridis')\n",
    "ax4.set_title('PINN Solution (Contour)')\n",
    "ax4.set_xlabel('x')\n",
    "ax4.set_ylabel('y')\n",
    "plt.colorbar(contour1, ax=ax4)\n",
    "\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "contour2 = ax5.contourf(X_np, Y_np, u_exact_np, levels=20, cmap='viridis')\n",
    "ax5.set_title('Exact Solution (Contour)')\n",
    "ax5.set_xlabel('x')\n",
    "ax5.set_ylabel('y')\n",
    "plt.colorbar(contour2, ax=ax5)\n",
    "\n",
    "# Training history\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "ax6.semilogy(trainer_2d.history['loss'], label='Total Loss')\n",
    "ax6.semilogy(trainer_2d.history['pde_loss'], label='PDE Loss')\n",
    "ax6.semilogy(trainer_2d.history['bc_loss'], label='BC Loss')\n",
    "ax6.set_xlabel('Epoch')\n",
    "ax6.set_ylabel('Loss')\n",
    "ax6.set_title('Training History')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute errors\n",
    "mse_2d = torch.mean((u_pred - u_exact)**2).item()\n",
    "rel_l2_2d = torch.norm(u_pred - u_exact) / torch.norm(u_exact)\n",
    "print(f\"2D Poisson - Mean Squared Error: {mse_2d:.2e}\")\n",
    "print(f\"2D Poisson - Relative L2 Error: {rel_l2_2d.item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example 3: Burgers' Equation {#example-3}\n",
    "\n",
    "Now let's solve a time-dependent nonlinear PDE - Burgers' equation:\n",
    "\n",
    "### Problem Setup\n",
    "$$\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2}$$\n",
    "\n",
    "with:\n",
    "- Domain: $x \\in [-1, 1]$, $t \\in [0, 1]$\n",
    "- Initial condition: $u(x, 0) = -\\sin(\\pi x)$\n",
    "- Boundary conditions: $u(-1, t) = u(1, t) = 0$\n",
    "- Viscosity: $\\nu = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Burgers_PINN:\n",
    "    \"\"\"\n",
    "    PINN for solving Burgers' equation.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, nu=0.01):\n",
    "        self.model = model\n",
    "        self.nu = nu\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        self.history = {'loss': [], 'pde_loss': [], 'ic_loss': [], 'bc_loss': []}\n",
    "    \n",
    "    def pde_loss(self, xt_collocation):\n",
    "        \"\"\"Compute PDE residual loss.\"\"\"\n",
    "        xt_collocation.requires_grad_(True)\n",
    "        u = self.model(xt_collocation)\n",
    "        \n",
    "        # Compute derivatives\n",
    "        u_x = compute_derivatives(u, xt_collocation, order=1)[0]  # âˆ‚u/âˆ‚x\n",
    "        u_t = compute_derivatives(u, xt_collocation, order=1)[1]  # âˆ‚u/âˆ‚t\n",
    "        u_xx = compute_derivatives(u, xt_collocation, order=2)[0]  # âˆ‚Â²u/âˆ‚xÂ²\n",
    "        \n",
    "        # Burgers' equation residual: âˆ‚u/âˆ‚t + uâˆ‚u/âˆ‚x - Î½âˆ‚Â²u/âˆ‚xÂ² = 0\n",
    "        residual = u_t + u * u_x - self.nu * u_xx\n",
    "        return torch.mean(residual**2)\n",
    "    \n",
    "    def initial_loss(self, x_initial, u_initial):\n",
    "        \"\"\"Compute initial condition loss.\"\"\"\n",
    "        u_pred = self.model(x_initial)\n",
    "        return torch.mean((u_pred - u_initial)**2)\n",
    "    \n",
    "    def boundary_loss(self, xt_boundary):\n",
    "        \"\"\"Compute boundary condition loss.\"\"\"\n",
    "        u_pred = self.model(xt_boundary)\n",
    "        return torch.mean(u_pred**2)\n",
    "    \n",
    "    def train(self, xt_collocation, x_initial, u_initial, xt_boundary, \n",
    "              epochs=5000, lambda_pde=1.0, lambda_ic=10.0, lambda_bc=10.0, \n",
    "              print_every=1000):\n",
    "        \"\"\"Train the PINN.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Compute losses\n",
    "            pde_loss = self.pde_loss(xt_collocation)\n",
    "            ic_loss = self.initial_loss(x_initial, u_initial)\n",
    "            bc_loss = self.boundary_loss(xt_boundary)\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = lambda_pde * pde_loss + lambda_ic * ic_loss + lambda_bc * bc_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Record history\n",
    "            self.history['loss'].append(total_loss.item())\n",
    "            self.history['pde_loss'].append(pde_loss.item())\n",
    "            self.history['ic_loss'].append(ic_loss.item())\n",
    "            self.history['bc_loss'].append(bc_loss.item())\n",
    "            \n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                print(f\"Epoch {epoch+1:5d}: Total = {total_loss.item():.6e}, \"\n",
    "                      f\"PDE = {pde_loss.item():.6e}, IC = {ic_loss.item():.6e}, \"\n",
    "                      f\"BC = {bc_loss.item():.6e}\")\n",
    "\n",
    "# Create Burgers' equation training data\n",
    "n_collocation = 2000\n",
    "x_col = torch.rand(n_collocation, 1) * 2 - 1  # x âˆˆ [-1, 1]\n",
    "t_col = torch.rand(n_collocation, 1)          # t âˆˆ [0, 1]\n",
    "xt_collocation = torch.cat([x_col, t_col], dim=1)\n",
    "\n",
    "# Initial condition: u(x, 0) = -sin(Ï€x)\n",
    "n_initial = 100\n",
    "x_initial = torch.linspace(-1, 1, n_initial).reshape(-1, 1)\n",
    "t_initial = torch.zeros_like(x_initial)\n",
    "xt_initial = torch.cat([x_initial, t_initial], dim=1)\n",
    "u_initial = -torch.sin(torch.pi * x_initial)\n",
    "\n",
    "# Boundary conditions: u(-1, t) = u(1, t) = 0\n",
    "n_boundary = 100\n",
    "t_boundary = torch.linspace(0, 1, n_boundary).reshape(-1, 1)\n",
    "x_left = torch.full_like(t_boundary, -1)\n",
    "x_right = torch.full_like(t_boundary, 1)\n",
    "xt_boundary = torch.cat([\n",
    "    torch.cat([x_left, t_boundary], dim=1),\n",
    "    torch.cat([x_right, t_boundary], dim=1)\n",
    "], dim=0)\n",
    "\n",
    "# Create model and trainer\n",
    "model_burgers = SimplePINN(input_dim=2, hidden_dim=64, output_dim=1, num_layers=6)\n",
    "trainer_burgers = Burgers_PINN(model_burgers, nu=0.01)\n",
    "\n",
    "print(\"Training Burgers' equation PINN...\")\n",
    "trainer_burgers.train(xt_collocation, xt_initial, u_initial, xt_boundary, epochs=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and visualize Burgers' equation results\n",
    "n_x, n_t = 100, 50\n",
    "x_test = torch.linspace(-1, 1, n_x)\n",
    "t_test = torch.linspace(0, 1, n_t)\n",
    "X_test, T_test = torch.meshgrid(x_test, t_test, indexing='ij')\n",
    "xt_test = torch.stack([X_test.flatten(), T_test.flatten()], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    u_pred = model_burgers(xt_test).reshape(n_x, n_t)\n",
    "\n",
    "# Convert to numpy\n",
    "X_np = X_test.numpy()\n",
    "T_np = T_test.numpy()\n",
    "u_pred_np = u_pred.numpy()\n",
    "\n",
    "# Create visualization\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(2, 3, 1, projection='3d')\n",
    "surf = ax1.plot_surface(X_np, T_np, u_pred_np, cmap='viridis', alpha=0.8)\n",
    "ax1.set_title('Burgers\\' Equation Solution')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('t')\n",
    "ax1.set_zlabel('u(x,t)')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "contour = ax2.contourf(X_np, T_np, u_pred_np, levels=20, cmap='viridis')\n",
    "ax2.set_title('Solution Evolution (Contour)')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('t')\n",
    "plt.colorbar(contour, ax=ax2)\n",
    "\n",
    "# Solution at different times\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "time_indices = [0, 10, 20, 30, 40, 49]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(time_indices)))\n",
    "for i, color in zip(time_indices, colors):\n",
    "    ax3.plot(x_test.numpy(), u_pred_np[:, i], color=color, \n",
    "             label=f't = {t_test[i]:.2f}', linewidth=2)\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('u(x,t)')\n",
    "ax3.set_title('Solution at Different Times')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Initial condition comparison\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "u_ic_pred = u_pred_np[:, 0]\n",
    "u_ic_exact = -np.sin(np.pi * x_test.numpy())\n",
    "ax4.plot(x_test.numpy(), u_ic_pred, 'b-', linewidth=2, label='PINN IC')\n",
    "ax4.plot(x_test.numpy(), u_ic_exact, 'r--', linewidth=2, label='Exact IC')\n",
    "ax4.set_xlabel('x')\n",
    "ax4.set_ylabel('u(x,0)')\n",
    "ax4.set_title('Initial Condition')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Training history\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "ax5.semilogy(trainer_burgers.history['loss'], label='Total Loss')\n",
    "ax5.semilogy(trainer_burgers.history['pde_loss'], label='PDE Loss')\n",
    "ax5.semilogy(trainer_burgers.history['ic_loss'], label='IC Loss')\n",
    "ax5.semilogy(trainer_burgers.history['bc_loss'], label='BC Loss')\n",
    "ax5.set_xlabel('Epoch')\n",
    "ax5.set_ylabel('Loss')\n",
    "ax5.set_title('Training History')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual analysis\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "# Compute PDE residual at test points\n",
    "xt_test.requires_grad_(True)\n",
    "u_test = model_burgers(xt_test)\n",
    "u_x = compute_derivatives(u_test, xt_test, order=1)[0]\n",
    "u_t = compute_derivatives(u_test, xt_test, order=1)[1]\n",
    "u_xx = compute_derivatives(u_test, xt_test, order=2)[0]\n",
    "residual = u_t + u_test * u_x - 0.01 * u_xx\n",
    "residual_np = residual.detach().numpy().reshape(n_x, n_t)\n",
    "\n",
    "im = ax6.imshow(np.abs(residual_np), aspect='auto', cmap='Reds', \n",
    "                extent=[-1, 1, 0, 1], origin='lower')\n",
    "ax6.set_title('PDE Residual Magnitude')\n",
    "ax6.set_xlabel('x')\n",
    "ax6.set_ylabel('t')\n",
    "plt.colorbar(im, ax=ax6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Burgers' equation - Max residual: {np.abs(residual_np).max():.2e}\")\n",
    "print(f\"Initial condition error: {np.mean((u_ic_pred - u_ic_exact)**2):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Topics {#advanced-topics}\n",
    "\n",
    "### 7.1 Adaptive Weights\n",
    "\n",
    "One challenge in training PINNs is balancing different loss components. Here's an implementation of adaptive weight balancing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveWeightPINN:\n",
    "    \"\"\"\n",
    "    PINN with adaptive loss weights based on the magnitude of gradients.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        self.weights = {'pde': 1.0, 'bc': 1.0, 'ic': 1.0}\n",
    "        self.alpha = 0.9  # Smoothing factor for weight updates\n",
    "    \n",
    "    def update_weights(self, losses):\n",
    "        \"\"\"\n",
    "        Update loss weights based on the rate of decrease of each loss component.\n",
    "        \"\"\"\n",
    "        # Compute gradients for each loss component\n",
    "        grad_norms = {}\n",
    "        \n",
    "        for key, loss in losses.items():\n",
    "            if key != 'total':\n",
    "                grad = torch.autograd.grad(loss, self.model.parameters(), \n",
    "                                         retain_graph=True, create_graph=False)\n",
    "                grad_norm = torch.norm(torch.cat([g.flatten() for g in grad]))\n",
    "                grad_norms[key] = grad_norm.item()\n",
    "        \n",
    "        # Update weights inversely proportional to gradient norms\n",
    "        max_grad = max(grad_norms.values())\n",
    "        for key in self.weights:\n",
    "            if key in grad_norms:\n",
    "                new_weight = max_grad / (grad_norms[key] + 1e-8)\n",
    "                self.weights[key] = self.alpha * self.weights[key] + (1 - self.alpha) * new_weight\n",
    "    \n",
    "    def compute_loss(self, data):\n",
    "        \"\"\"\n",
    "        Compute weighted loss with adaptive weights.\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        \n",
    "        # Compute individual losses (implementation depends on specific PDE)\n",
    "        # This is a template - would be implemented for specific problems\n",
    "        \n",
    "        return losses\n",
    "\n",
    "print(\"Adaptive weight balancing helps manage competing loss terms in complex PDEs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Fourier Neural Operators Integration\n",
    "\n",
    "PINNs can be combined with Fourier Neural Operators for better performance on periodic problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierFeaturePINN(nn.Module):\n",
    "    \"\"\"\n",
    "    PINN with Fourier feature encoding for better high-frequency representation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1, hidden_dim=256, output_dim=1, \n",
    "                 fourier_dim=128, sigma=1.0):\n",
    "        super(FourierFeaturePINN, self).__init__()\n",
    "        \n",
    "        # Random Fourier features\n",
    "        self.B = nn.Parameter(torch.randn(input_dim, fourier_dim) * sigma)\n",
    "        self.B.requires_grad = False\n",
    "        \n",
    "        # Network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(2 * fourier_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def fourier_features(self, x):\n",
    "        \"\"\"Apply random Fourier features.\"\"\"\n",
    "        x_proj = torch.matmul(x, self.B) * 2 * np.pi\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_fourier = self.fourier_features(x)\n",
    "        return self.network(x_fourier)\n",
    "\n",
    "# Test Fourier feature PINN\n",
    "fourier_model = FourierFeaturePINN(input_dim=1, fourier_dim=64)\n",
    "test_input = torch.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "test_output = fourier_model(test_input)\n",
    "print(f\"Fourier PINN output shape: {test_output.shape}\")\n",
    "print(\"Fourier features help capture high-frequency components in solutions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Transfer Learning for PINNs\n",
    "\n",
    "Pre-trained PINNs can be fine-tuned for similar problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning_example():\n",
    "    \"\"\"\n",
    "    Example of transfer learning with PINNs.\n",
    "    \"\"\"\n",
    "    # Assume we have a pre-trained model for Poisson equation\n",
    "    pretrained_model = SimplePINN(input_dim=1, hidden_dim=32, output_dim=1)\n",
    "    \n",
    "    # Fine-tune for a similar problem (e.g., Poisson with different boundary conditions)\n",
    "    # Strategy 1: Freeze early layers, train only output layers\n",
    "    for param in pretrained_model.network[:-2].parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Strategy 2: Lower learning rate for pretrained layers\n",
    "    pretrained_params = list(pretrained_model.network[:-2].parameters())\n",
    "    new_params = list(pretrained_model.network[-2:].parameters())\n",
    "    \n",
    "    optimizer = optim.Adam([\n",
    "        {'params': pretrained_params, 'lr': 1e-5},\n",
    "        {'params': new_params, 'lr': 1e-3}\n",
    "    ])\n",
    "    \n",
    "    print(\"Transfer learning can accelerate training for similar PDE problems.\")\n",
    "    print(\"Strategies include freezing layers or using different learning rates.\")\n",
    "\n",
    "transfer_learning_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion {#conclusion}\n",
    "\n",
    "In this tutorial, we've covered:\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **PINNs embed physics directly**: PDEs become part of the loss function\n",
    "2. **Automatic differentiation is crucial**: Enables computation of derivatives for PDE residuals\n",
    "3. **Multiple loss components**: Balance PDE residuals, boundary conditions, and data fitting\n",
    "4. **Architecture matters**: Different network designs for different problem types\n",
    "\n",
    "### Best Practices:\n",
    "- **Proper weight initialization**: Use Xavier or He initialization\n",
    "- **Loss balancing**: Carefully balance different loss components\n",
    "- **Sampling strategy**: Use appropriate collocation point sampling\n",
    "- **Activation functions**: Tanh often works well for PINNs\n",
    "- **Monitoring training**: Watch all loss components, not just total loss\n",
    "\n",
    "### Applications:\n",
    "- Fluid dynamics (Navier-Stokes equations)\n",
    "- Heat transfer problems\n",
    "- Wave propagation\n",
    "- Inverse problems\n",
    "- Multi-physics simulations\n",
    "\n",
    "### Future Directions:\n",
    "- **Improved architectures**: ResNets, attention mechanisms\n",
    "- **Better optimization**: L-BFGS, adaptive learning rates\n",
    "- **Uncertainty quantification**: Bayesian PINNs\n",
    "- **Multi-scale problems**: Hierarchical approaches\n",
    "\n",
    "PINNs represent a paradigm shift in computational physics, offering a flexible and powerful approach to solving PDEs. As the field continues to evolve, we can expect even more sophisticated techniques and broader applications.\n",
    "\n",
    "### Next Steps:\n",
    "1. Try implementing PINNs for your own PDE problems\n",
    "2. Experiment with different architectures and loss formulations\n",
    "3. Explore the other methods in this repository (DeepONet, FNO, Transformers)\n",
    "4. Consider hybrid approaches combining multiple techniques\n",
    "\n",
    "Happy learning and coding! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
