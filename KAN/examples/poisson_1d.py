"""
ä½¿ç”¨ KAN æ±‚è§£ 1D Poisson æ–¹ç¨‹

é—®é¢˜æè¿°:
    -dÂ²u/dxÂ² = f(x),  x âˆˆ [-1, 1]
    è¾¹ç•Œæ¡ä»¶: u(-1) = 0, u(1) = 0
    
æœ¬ä¾‹ä¸­å– f(x) = sin(Ï€x)
è§£æè§£: u(x) = sin(Ï€x) / Ï€Â²

ç‰¹ç‚¹: KAN åœ¨æ±‚è§£æ­¤ç±»å…‰æ»‘é—®é¢˜æ—¶è¡¨ç°ä¼˜å¼‚
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import KANPDE

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}\n")


# ==============================================================================
# 1. è§£æè§£
# ==============================================================================

def analytical_solution(x):
    """
    Poisson æ–¹ç¨‹è§£æè§£: u(x) = sin(Ï€x) / Ï€Â²
    """
    return np.sin(np.pi * x) / (np.pi ** 2)


def source_term(x):
    """
    æºé¡¹: f(x) = sin(Ï€x)
    """
    return torch.sin(np.pi * x)


# ==============================================================================
# 2. æ•°æ®å‡†å¤‡
# ==============================================================================

def prepare_training_data(n_interior=100, n_boundary=2):
    """
    å‡†å¤‡è®­ç»ƒæ•°æ®
    
    å‚æ•°:
        n_interior: å†…éƒ¨é…ç‚¹æ•°é‡
        n_boundary: è¾¹ç•Œç‚¹æ•°é‡
    
    è¿”å›:
        x_interior: å†…éƒ¨ç‚¹
        x_boundary: è¾¹ç•Œç‚¹
        u_boundary: è¾¹ç•Œå€¼
    """
    # å†…éƒ¨é…ç‚¹ (ç”¨äºè®¡ç®— PDE æ®‹å·®)
    x_interior = torch.linspace(-1, 1, n_interior).unsqueeze(1).to(device)
    x_interior.requires_grad_(True)
    
    # è¾¹ç•Œç‚¹
    x_boundary = torch.tensor([[-1.0], [1.0]], device=device)
    u_boundary = torch.tensor([[0.0], [0.0]], device=device)
    
    return x_interior, x_boundary, u_boundary


# ==============================================================================
# 3. æŸå¤±å‡½æ•°
# ==============================================================================

def compute_pde_loss(model, x_interior, x_boundary, u_boundary, lambda_reg=1e-5):
    """
    è®¡ç®—æ€»æŸå¤±
    
    L_total = L_pde + L_bc + Î» * L_reg
    
    å…¶ä¸­:
    - L_pde: PDE æ®‹å·®æŸå¤± (å†…éƒ¨ç‚¹)
    - L_bc: è¾¹ç•Œæ¡ä»¶æŸå¤±
    - L_reg: æ­£åˆ™åŒ–æŸå¤± (B-spline ç³»æ•°)
    """
    # ========== PDE æ®‹å·®æŸå¤± ==========
    # è®¡ç®— u å’Œ u''
    u = model(x_interior)
    
    # ä¸€é˜¶å¯¼æ•° du/dx
    u_x = torch.autograd.grad(
        u, x_interior,
        grad_outputs=torch.ones_like(u),
        create_graph=True,
        retain_graph=True
    )[0]
    
    # äºŒé˜¶å¯¼æ•° dÂ²u/dxÂ²
    u_xx = torch.autograd.grad(
        u_x, x_interior,
        grad_outputs=torch.ones_like(u_x),
        create_graph=True,
        retain_graph=True
    )[0]
    
    # PDE: -u_xx = f(x)
    f = source_term(x_interior)
    pde_residual = -u_xx - f
    loss_pde = torch.mean(pde_residual ** 2)
    
    # ========== è¾¹ç•Œæ¡ä»¶æŸå¤± ==========
    u_b = model(x_boundary)
    loss_bc = torch.mean((u_b - u_boundary) ** 2)
    
    # ========== æ­£åˆ™åŒ–æŸå¤± ==========
    loss_reg = model.regularization_loss()
    
    # ========== æ€»æŸå¤± ==========
    total_loss = loss_pde + loss_bc + lambda_reg * loss_reg
    
    return total_loss, {
        'pde': loss_pde.item(),
        'bc': loss_bc.item(),
        'reg': loss_reg.item()
    }


# ==============================================================================
# 4. è®­ç»ƒå‡½æ•°
# ==============================================================================

def train_kan(model, x_interior, x_boundary, u_boundary, 
              epochs=5000, lr=1e-3, lambda_reg=1e-5):
    """
    è®­ç»ƒ KAN æ¨¡å‹
    """
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.8, patience=500, verbose=False
    )
    
    history = {'total': [], 'pde': [], 'bc': [], 'reg': []}
    
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print(f"   Epochs: {epochs}, å­¦ä¹ ç‡: {lr}, æ­£åˆ™åŒ–: {lambda_reg}\n")
    
    for epoch in range(epochs):
        optimizer.zero_grad()
        
        # è®¡ç®—æŸå¤±
        loss, loss_dict = compute_pde_loss(
            model, x_interior, x_boundary, u_boundary, lambda_reg
        )
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        scheduler.step(loss)
        
        # è®°å½•å†å²
        history['total'].append(loss.item())
        history['pde'].append(loss_dict['pde'])
        history['bc'].append(loss_dict['bc'])
        history['reg'].append(loss_dict['reg'])
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 1000 == 0 or epoch == 0:
            print(f"Epoch {epoch+1:5d}/{epochs} | "
                  f"Loss: {loss.item():.2e} | "
                  f"PDE: {loss_dict['pde']:.2e} | "
                  f"BC: {loss_dict['bc']:.2e} | "
                  f"LR: {optimizer.param_groups[0]['lr']:.2e}")
    
    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    return history


# ==============================================================================
# 5. è¯„ä¼°å’Œå¯è§†åŒ–
# ==============================================================================

def evaluate_and_plot(model, history):
    """
    è¯„ä¼°æ¨¡å‹å¹¶å¯è§†åŒ–ç»“æœ
    """
    model.eval()
    
    # æµ‹è¯•ç‚¹
    x_test = torch.linspace(-1, 1, 200).unsqueeze(1).to(device)
    
    with torch.no_grad():
        u_pred = model(x_test).cpu().numpy()
    
    x_test_np = x_test.cpu().numpy()
    u_exact = analytical_solution(x_test_np)
    
    # è®¡ç®—è¯¯å·®
    error = np.abs(u_pred - u_exact)
    l2_error = np.linalg.norm(error) / np.linalg.norm(u_exact)
    max_error = np.max(error)
    
    print(f"\nğŸ“Š è¯¯å·®åˆ†æ:")
    print(f"   ç›¸å¯¹ L2 è¯¯å·®: {l2_error:.4e}")
    print(f"   æœ€å¤§ç»å¯¹è¯¯å·®: {max_error:.4e}")
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. è§£çš„å¯¹æ¯”
    axes[0, 0].plot(x_test_np, u_exact, 'b-', linewidth=2, label='è§£æè§£')
    axes[0, 0].plot(x_test_np, u_pred, 'r--', linewidth=2, label='KAN é¢„æµ‹')
    axes[0, 0].set_xlabel('x')
    axes[0, 0].set_ylabel('u(x)')
    axes[0, 0].set_title('Poisson æ–¹ç¨‹è§£: KAN vs è§£æè§£')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. ç»å¯¹è¯¯å·®
    axes[0, 1].plot(x_test_np, error, 'g-', linewidth=2)
    axes[0, 1].set_xlabel('x')
    axes[0, 1].set_ylabel('|u_KAN - u_exact|')
    axes[0, 1].set_title(f'ç»å¯¹è¯¯å·® (Max: {max_error:.2e})')
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].set_yscale('log')
    
    # 3. è®­ç»ƒæŸå¤±æ›²çº¿
    epochs = range(1, len(history['total']) + 1)
    axes[1, 0].semilogy(epochs, history['total'], 'b-', label='æ€»æŸå¤±', alpha=0.8)
    axes[1, 0].semilogy(epochs, history['pde'], 'r-', label='PDE æŸå¤±', alpha=0.6)
    axes[1, 0].semilogy(epochs, history['bc'], 'g-', label='BC æŸå¤±', alpha=0.6)
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Loss (log scale)')
    axes[1, 0].set_title('è®­ç»ƒæŸå¤±æ›²çº¿')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. è¯¯å·®åˆ†å¸ƒ
    axes[1, 1].hist(error.flatten(), bins=50, color='purple', alpha=0.7, edgecolor='black')
    axes[1, 1].set_xlabel('ç»å¯¹è¯¯å·®')
    axes[1, 1].set_ylabel('é¢‘æ¬¡')
    axes[1, 1].set_title('è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾')
    axes[1, 1].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('kan_poisson_results.png', dpi=150, bbox_inches='tight')
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: kan_poisson_results.png")
    plt.show()
    
    return l2_error


# ==============================================================================
# 6. ä¸»ç¨‹åº
# ==============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("KAN æ±‚è§£ 1D Poisson æ–¹ç¨‹")
    print("PDE: -u'' = sin(Ï€x),  x âˆˆ [-1, 1]")
    print("BC:  u(-1) = u(1) = 0")
    print("=" * 70)
    
    # ========== åˆ›å»ºæ¨¡å‹ ==========
    print("\n[1/4] åˆ›å»º KAN æ¨¡å‹...")
    model = KANPDE(
        layers=[1, 16, 16, 1],  # 1è¾“å…¥ -> 16 -> 16 -> 1è¾“å‡º
        grid_size=5,            # B-spline ç½‘æ ¼å¤§å°
        spline_order=3,         # ä¸‰æ¬¡æ ·æ¡
        grid_range=(-1, 1)      # è¾“å…¥èŒƒå›´
    ).to(device)
    
    print(f"   æ¨¡å‹ç»“æ„: {[1, 16, 16, 1]}")
    print(f"   å‚æ•°é‡: {model.count_parameters():,}")
    
    # ========== å‡†å¤‡æ•°æ® ==========
    print("\n[2/4] å‡†å¤‡è®­ç»ƒæ•°æ®...")
    x_interior, x_boundary, u_boundary = prepare_training_data(
        n_interior=100,
        n_boundary=2
    )
    print(f"   å†…éƒ¨é…ç‚¹: {x_interior.shape[0]}")
    print(f"   è¾¹ç•Œç‚¹: {x_boundary.shape[0]}")
    
    # ========== è®­ç»ƒæ¨¡å‹ ==========
    print("\n[3/4] è®­ç»ƒæ¨¡å‹...")
    history = train_kan(
        model, x_interior, x_boundary, u_boundary,
        epochs=5000,
        lr=1e-3,
        lambda_reg=1e-5
    )
    
    # ========== è¯„ä¼°å’Œå¯è§†åŒ– ==========
    print("\n[4/4] è¯„ä¼°å’Œå¯è§†åŒ–...")
    l2_error = evaluate_and_plot(model, history)
    
    print("\n" + "=" * 70)
    print(f"âœ… å®éªŒå®Œæˆ! ç›¸å¯¹ L2 è¯¯å·®: {l2_error:.4e}")
    print("=" * 70)
