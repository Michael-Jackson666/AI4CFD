# Deep Operator Networks (DeepONet)

Deep Operator Networks (DeepONet) are neural networks designed to learn operators that map between infinite-dimensional function spaces. Unlike traditional neural networks that map finite-dimensional vectors to vectors, DeepONet can learn mappings from functions to functions, making them particularly powerful for solving families of PDEs and operator learning problems.

DeepONet æ˜¯ç”¨äºå­¦ä¹ æ— ç©·ç»´å‡½æ•°ç©ºé—´ä¹‹é—´ç®—å­æ˜ å°„çš„ç¥ç»ç½‘ç»œã€‚ä¸ä¼ ç»Ÿç¥ç»ç½‘ç»œä¸åŒï¼ŒDeepONet å¯ä»¥å­¦ä¹ ä»å‡½æ•°åˆ°å‡½æ•°çš„æ˜ å°„ï¼Œç‰¹åˆ«é€‚ç”¨äºæ±‚è§£ PDE æ—å’Œç®—å­å­¦ä¹ é—®é¢˜ã€‚

## ğŸ“¦ å®‰è£… (Installation)

### Prerequisites
- Python 3.8+
- PyTorch 1.10+
- CUDA (optional, for GPU acceleration)

### Basic Installation
```bash
# Clone the repository
git clone https://github.com/your-repo/AI4CFD.git
cd AI4CFD/DeepONet

# Install PyTorch (CPU version)
pip install torch torchvision torchaudio

# For GPU support, visit: https://pytorch.org/get-started/locally/

# Install other dependencies
pip install numpy scipy matplotlib jupyter
```

### Verify Installation
```bash
python -c "import torch; print('PyTorch version:', torch.__version__)"
python operator_learning_pure.py  # Run simple example
```

## ğŸ¯ Key Concepts

### What is DeepONet?
DeepONet learns operator mappings of the form:
```
G: U â†’ V
```
where U and V are function spaces. For example:
- **PDE Solution Operator**: G maps initial/boundary conditions to PDE solutions
- **Parameter-to-Solution Map**: G maps PDE parameters to solutions
- **Time Evolution Operator**: G maps current state to future state

### Architecture
DeepONet consists of two sub-networks:
1. **Branch Network**: Encodes the input function at sensor locations
2. **Trunk Network**: Encodes the query locations where we want to evaluate the output
3. **Combination**: Outputs are combined via inner product: `G(u)(y) = Î£áµ¢ báµ¢(u) * táµ¢(y)`

### Key Advantages
- **Universal approximation**: Can approximate any continuous operator
- **Generalization**: Trained on one set of functions, generalizes to new functions
- **Efficiency**: No need to retrain for new input functions
- **Theoretical foundation**: Based on universal approximation theorem for operators

## ğŸ“ ç›®å½•ç»“æ„ (Directory Structure)

```
DeepONet/
â”œâ”€â”€ README.md                          # æœ¬æ–‡æ¡£
â”œâ”€â”€ models.py                          # DeepONet æ¶æ„å’Œå˜ä½“
â”œâ”€â”€ operators.py                       # å¸¸è§ç®—å­å®šä¹‰å’Œç¤ºä¾‹
â”œâ”€â”€ operator_learning_pure.py         # çº¯ PyTorch å®ç°çš„ç®—å­å­¦ä¹ 
â”‚
â”œâ”€â”€ tutorial/                          # ğŸ“š æ•™ç¨‹ç¬”è®°æœ¬
â”‚   â””â”€â”€ operator_learning_torch.ipynb # PyTorch DeepONet å®Œæ•´æ•™ç¨‹
â”‚
â””â”€â”€ vp_system/                         # ğŸŒŒ Vlasov-Poisson ç®—å­å­¦ä¹ 
    â”œâ”€â”€ README.md                      # VP ç®—å­å­¦ä¹ è¯¦ç»†æ–‡æ¡£
    â”œâ”€â”€ main.py                        # ä¸»è®­ç»ƒè„šæœ¬
    â”œâ”€â”€ vp_operator.py                 # VP ç³»ç»Ÿç®—å­å®šä¹‰
    â”œâ”€â”€ data_generate.py               # VP è®­ç»ƒæ•°æ®ç”Ÿæˆ
    â”œâ”€â”€ transformer.py                 # Transformer æ¶æ„
    â””â”€â”€ visualization.py               # ç»“æœå¯è§†åŒ–å·¥å…·
```

## ğŸ“ æ ¸å¿ƒæ–‡ä»¶è¯´æ˜ (Core Files Description)

### `models.py`
DeepONet æ¶æ„å’Œå˜ä½“å®ç°ã€‚

**åŒ…å«çš„æ¨¡å‹**:
- **StandardDeepONet**: æ ‡å‡† DeepONetï¼ˆBranch + Trunk ç½‘ç»œï¼‰
- **PODDeepONet**: åŸºäº PODï¼ˆæœ¬å¾æ­£äº¤åˆ†è§£ï¼‰çš„ DeepONet
- **DeepONet with Attention**: æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºç‰ˆæœ¬
- **Residual DeepONet**: å¸¦æ®‹å·®è¿æ¥çš„ DeepONet

### `operators.py`
å¸¸è§ç®—å­å®šä¹‰å’Œæ•°æ®ç”Ÿæˆå‡½æ•°ã€‚

**æ”¯æŒçš„ç®—å­**:
- **åå¯¼æ•°ç®—å­**: $G: f(x) \rightarrow \int f(x)dx$
- **çƒ­æ–¹ç¨‹ç®—å­**: $G: u_0(x) \rightarrow u(x,t)$
- **Darcy æµç®—å­**: $G: \kappa(x,y) \rightarrow p(x,y)$
- **Burgers æ–¹ç¨‹ç®—å­**: éçº¿æ€§å¯¹æµæ‰©æ•£ç®—å­
- **æ³Šæ¾æ–¹ç¨‹ç®—å­**: $G: f(x) \rightarrow u(x)$ å…¶ä¸­ $\nabla^2 u = f$

### `operator_learning_pure.py`
ä½¿ç”¨çº¯ PyTorch å®ç°çš„ç®—å­å­¦ä¹ å®Œæ•´ç¤ºä¾‹ã€‚

**åŠŸèƒ½**:
- ä»é›¶å¼€å§‹æ„å»º DeepONet
- æ•°æ®ç”Ÿæˆå’Œé¢„å¤„ç†
- è®­ç»ƒå¾ªç¯å’ŒéªŒè¯
- ç»“æœå¯è§†åŒ–å’Œè¯¯å·®åˆ†æ
- é€‚åˆå­¦ä¹  DeepONet çš„å†…éƒ¨æœºåˆ¶

## ï¿½ æ•™ç¨‹ç¬”è®°æœ¬ (Tutorial Notebook)

### `tutorial/operator_learning_torch.ipynb`
**å®Œæ•´çš„ PyTorch DeepONet æ•™ç¨‹**

**å†…å®¹æ¶µç›–**:
- ğŸ“– **DeepONet ç†è®ºåŸºç¡€**: ç®—å­å­¦ä¹ çš„æ•°å­¦åŸç†å’Œé€šç”¨é€¼è¿‘å®šç†
- ğŸ—ï¸ **æ¶æ„è®¾è®¡**: Branch ç½‘ç»œå’Œ Trunk ç½‘ç»œçš„è¯¦ç»†è§£æ
- ğŸ’» **ä»£ç å®ç°**: ä»é›¶å¼€å§‹ç”¨ PyTorch æ„å»º DeepONet
- ğŸ“Š **å®é™…æ¡ˆä¾‹**: 
  - åå¯¼æ•°ç®—å­å­¦ä¹ 
  - çƒ­æ–¹ç¨‹è§£ç®—å­
  - Darcy æµç®—å­
- ğŸ¯ **è®­ç»ƒæŠ€å·§**: æ•°æ®ç”Ÿæˆç­–ç•¥ã€æŸå¤±å‡½æ•°è®¾è®¡ã€è¶…å‚æ•°è°ƒä¼˜
- ğŸ“ˆ **ç»“æœåˆ†æ**: è¯¯å·®è¯„ä¼°ã€æ³›åŒ–èƒ½åŠ›æµ‹è¯•ã€å¯è§†åŒ–æŠ€æœ¯

**é€‚åˆäººç¾¤**: 
- å¸Œæœ›æ·±å…¥ç†è§£ DeepONet åŸç†çš„ç ”ç©¶è€…
- éœ€è¦è‡ªå®šä¹‰ç®—å­å­¦ä¹ æ¨¡å‹çš„å¼€å‘è€…
- å­¦ä¹ ç§‘å­¦æœºå™¨å­¦ä¹ ï¼ˆSciMLï¼‰çš„å­¦ç”Ÿ

## ï¿½ğŸš€ å¿«é€Ÿå¼€å§‹ (Quick Start)

### Running the Tutorial
```bash
# PyTorch DeepONet å®Œæ•´æ•™ç¨‹
jupyter notebook tutorial/operator_learning_torch.ipynb
```

### Running Pure PyTorch Implementation
```bash
# è¿è¡Œçº¯ PyTorch å®ç°ç¤ºä¾‹
python operator_learning_pure.py
```

### ğŸŒŒ Vlasov-Poisson Operator Learning (vp_system/)

For Vlasov-Poisson operator learning applications (å­¦ä¹  VP ç³»ç»Ÿçš„è§£ç®—å­):

```bash
cd vp_system/

# 1. ç”Ÿæˆè®­ç»ƒæ•°æ®
python data_generate.py

# 2. è®­ç»ƒ DeepONet æ¨¡å‹
python main.py

# 3. å¯è§†åŒ–ç»“æœ
python visualization.py
```

**åŠŸèƒ½ç‰¹ç‚¹**:
- **ç®—å­å­¦ä¹ **: å­¦ä¹ ä»åˆå§‹åˆ†å¸ƒ $f_0(x,v)$ åˆ°æ¼”åŒ–ååˆ†å¸ƒ $f(x,v,t)$ çš„æ˜ å°„
- **æ•°æ®ç”Ÿæˆ**: è‡ªåŠ¨ç”Ÿæˆå¤šç»„ä¸åŒåˆå§‹æ¡ä»¶çš„ VP ç³»ç»Ÿæ±‚è§£æ•°æ®
- **Transformer æ¶æ„**: æ”¯æŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ DeepONet å˜ä½“
- **å¯è§†åŒ–å·¥å…·**: ç›¸ç©ºé—´ã€ç”µåœºã€åˆ†å¸ƒå‡½æ•°çš„ç»¼åˆç»˜å›¾

**Key Files**:
- `vp_operator.py`: VP ç³»ç»Ÿç®—å­å®šä¹‰å’Œ DeepONet å®ç°
- `data_generate.py`: ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆå¤šç»„åˆå§‹æ¡ä»¶çš„ VP æ¼”åŒ–ï¼‰
- `main.py`: å®Œæ•´çš„è®­ç»ƒå’Œæµ‹è¯•æµç¨‹
- `transformer.py`: Transformer-based DeepONet æ¶æ„
- `visualization.py`: ç»“æœå¯è§†åŒ–ï¼ˆç›¸ç©ºé—´åŠ¨ç”»ã€è¯¯å·®åˆ†æç­‰ï¼‰

è¯¦è§ `vp_system/README.md` äº†è§£æ›´å¤šç»†èŠ‚ã€‚

**æŠ€æœ¯äº®ç‚¹**:
- âš¡ **é«˜æ•ˆæ•°æ®ç”Ÿæˆ**: è‡ªåŠ¨åŒ–ç”Ÿæˆå¤šç§åˆå§‹æ¡ä»¶çš„ VP æ¼”åŒ–è½¨è¿¹
- ğŸ§  **å…ˆè¿›æ¶æ„**: æ”¯æŒä¼ ç»Ÿ MLP å’Œ Transformer ä¸¤ç§ DeepONet å®ç°
- ğŸ¨ **ä¸°å¯Œå¯è§†åŒ–**: ç›¸ç©ºé—´å¯†åº¦ã€ç”µåœºåˆ†å¸ƒã€æ—¶é—´æ¼”åŒ–åŠ¨ç”»
- ğŸ“Š **å®šé‡è¯„ä¼°**: L2 è¯¯å·®ã€ç›¸å¯¹è¯¯å·®ã€èƒ½é‡å®ˆæ’æ£€éªŒ
- ğŸ”„ **ç«¯åˆ°ç«¯æµç¨‹**: ä»æ•°æ®ç”Ÿæˆåˆ°æ¨¡å‹è®­ç»ƒåˆ°ç»“æœåˆ†æçš„å®Œæ•´ç®¡é“

**ä¸ä¼ ç»Ÿæ±‚è§£å™¨å¯¹æ¯”**:
| ç‰¹æ€§ | ä¼ ç»Ÿæ•°å€¼æ±‚è§£å™¨ | DeepONet ç®—å­å­¦ä¹  |
|------|---------------|------------------|
| å•æ¬¡æ±‚è§£æ—¶é—´ | åˆ†é’Ÿ-å°æ—¶ | æ¯«ç§’çº§ |
| å‚æ•°æ‰«æ | æ¯ç»„å‚æ•°é‡æ–°è®¡ç®— | ä¸€æ¬¡å‰å‘ä¼ æ’­ |
| å†…å­˜å ç”¨ | é«˜ï¼ˆç½‘æ ¼æ•°æ®ï¼‰ | ä½ï¼ˆç½‘ç»œå‚æ•°ï¼‰ |
| é€‚ç”¨åœºæ™¯ | é«˜ç²¾åº¦å•æ¬¡è®¡ç®— | å¿«é€Ÿå¤šæ¬¡é¢„æµ‹ |

## ğŸ“Š ç¤ºä¾‹åº”ç”¨ (Example Applications)

### 1. çƒ­æ–¹ç¨‹ç®—å­ (Heat Equation Operator)
å­¦ä¹ ä»åˆå§‹æ¸©åº¦åˆ†å¸ƒåˆ°ä»»æ„æ—¶åˆ»æ¸©åº¦åˆ†å¸ƒçš„ç®—å­æ˜ å°„ï¼š
```
G: uâ‚€(x) â†’ u(x,t)
```
**åº”ç”¨**: ç¬æ€çƒ­ä¼ å¯¼ã€æ¸©åº¦åœºé¢„æµ‹

### 2. Darcy æµç®—å­ (Darcy Flow Operator)
å­¦ä¹ ä»æ¸—é€ç‡åœºåˆ°å‹åŠ›/é€Ÿåº¦åœºçš„æ˜ å°„ï¼š
```
G: Îº(x,y) â†’ p(x,y)
```
**åº”ç”¨**: å¤šå­”ä»‹è´¨æµåŠ¨ã€åœ°ä¸‹æ°´æ¨¡æ‹Ÿã€æ²¹è—å·¥ç¨‹

### 3. åå¯¼æ•°ç®—å­ (Antiderivative Operator)
å­¦ä¹ ä»å‡½æ•°åˆ°å…¶ä¸å®šç§¯åˆ†çš„æ˜ å°„ï¼š
```
G: f(x) â†’ âˆ«f(x)dx
```
**åº”ç”¨**: æ•°å€¼ç§¯åˆ†ã€ç¬¦å·è®¡ç®—çš„ç¥ç»ç½‘ç»œæ›¿ä»£

### 4. Burgers æ–¹ç¨‹ç®—å­ (Burgers' Equation Operator)
å­¦ä¹ éçº¿æ€§å¯¹æµæ‰©æ•£æ–¹ç¨‹çš„è§£ç®—å­ï¼š
```
G: uâ‚€(x) â†’ u(x,t)  å…¶ä¸­  âˆ‚u/âˆ‚t + uâˆ‚u/âˆ‚x = Î½âˆ‚Â²u/âˆ‚xÂ²
```
**åº”ç”¨**: æ¿€æ³¢ä¼ æ’­ã€äº¤é€šæµæ¨¡æ‹Ÿã€æ¹æµå»ºæ¨¡

### 5. **Vlasov-Poisson ç®—å­** ğŸŒŒ (vp_system/)
å­¦ä¹ ç­‰ç¦»å­ä½“åŠ¨ç†å­¦æ–¹ç¨‹çš„æ¼”åŒ–ç®—å­ï¼š
```
G: fâ‚€(x,v) â†’ f(x,v,t)
```
**åº”ç”¨**: 
- åŒæµä¸ç¨³å®šæ€§ï¼ˆTwo-stream instabilityï¼‰æ¨¡æ‹Ÿ
- ç­‰ç¦»å­ä½“åŠ¨åŠ›å­¦é¢„æµ‹
- ç›¸ç©ºé—´æ¼”åŒ–å­¦ä¹ 
- ç”µåœºè‡ªæ´½è®¡ç®—

è¿™æ˜¯æœ¬é¡¹ç›®çš„**é«˜çº§åº”ç”¨ç¤ºä¾‹**ï¼Œå±•ç¤ºäº† DeepONet åœ¨å¤æ‚ç‰©ç†ç³»ç»Ÿä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚

## ğŸ”§ Implementation Details

### Training Data Generation
DeepONet requires:
1. **Input functions**: Sampled at sensor locations
2. **Output functions**: Evaluated at query locations
3. **Sensor locations**: Fixed points where input functions are observed
4. **Query locations**: Points where output is evaluated

### Network Architecture
```python
# Branch network: processes input function values
branch_net = MLP(input_dim=sensor_size, output_dim=p)

# Trunk network: processes query coordinates  
trunk_net = MLP(input_dim=coord_dim, output_dim=p)

# Output: inner product
output = torch.sum(branch_output * trunk_output, dim=-1)
```

### Loss Function
```python
loss = MSE(predicted_output, true_output) + regularization_terms
```

## ğŸ“š Mathematical Background

For a continuous operator G: U â†’ V between Banach spaces, DeepONet approximates:

$$G(u)(y) = \sum_{i=1}^p b_i(u) \cdot t_i(y) + b_0$$

where:
- $b_i(u)$ are the branch network outputs (depend on input function u)
- $t_i(y)$ are the trunk network outputs (depend on query location y)
- $p$ is the dimension of the latent space

### Universal Approximation
**Theorem**: If the branch network can approximate any continuous functional and the trunk network can approximate any continuous function, then DeepONet can approximate any continuous operator.

## ğŸ¯ Training Strategies

### 1. Standard Training
- Generate diverse input functions
- Sample query points uniformly
- Use standard MSE loss

### 2. Physics-Informed Training
- Add PDE residual loss
- Enforce boundary conditions
- Include conservation laws

### 3. Multi-Fidelity Training
- Use data from multiple resolutions
- Transfer learning between fidelities
- Adaptive sampling strategies

### 4. Residual Learning
- Learn residuals from simple operators
- Improve accuracy on complex problems
- Reduce training time

## ğŸ”— åœ¨ CFD ä¸­çš„åº”ç”¨ (Applications in CFD)

### æµä½“æµåŠ¨ç®—å­ (Fluid Flow Operators)
- **Navier-Stokes ç®—å­**: è¾¹ç•Œæ¡ä»¶ â†’ é€Ÿåº¦/å‹åŠ›åœº
  - åº”ç”¨: å¿«é€Ÿæµåœºé¢„æµ‹ã€å½¢çŠ¶ä¼˜åŒ–ã€å®æ—¶æ¨¡æ‹Ÿ
- **æ¹æµå»ºæ¨¡**: å­¦ä¹ äºšç½‘æ ¼å°ºåº¦æ¨¡å‹ç®—å­
  - åº”ç”¨: LES/RANS æ¨¡å‹çš„æ•°æ®é©±åŠ¨æ›¿ä»£
- **å½¢çŠ¶ä¼˜åŒ–**: å‡ ä½•å‚æ•° â†’ æµåŠ¨ç‰¹æ€§
  - åº”ç”¨: ç¿¼å‹è®¾è®¡ã€ç®¡é“ä¼˜åŒ–

### ä¼ çƒ­ç®—å­ (Heat Transfer)
- **çƒ­ä¼ å¯¼ç®—å­**: çƒ­ç‰©æ€§å‚æ•° â†’ æ¸©åº¦åœº
  - åº”ç”¨: ææ–™è®¾è®¡ã€çƒ­ç®¡ç†ç³»ç»Ÿ
- **å¯¹æµä¼ çƒ­**: å­¦ä¹ æ¢çƒ­ç³»æ•°ç®—å­
  - åº”ç”¨: å†·å´ç³»ç»Ÿè®¾è®¡ã€çƒ­äº¤æ¢å™¨ä¼˜åŒ–
- **è¾å°„ä¼ çƒ­**: å¤æ‚è¾å°„ä¼ é€’ç®—å­å»ºæ¨¡
  - åº”ç”¨: é«˜æ¸©ç³»ç»Ÿã€ç‡ƒçƒ§æ¨¡æ‹Ÿ

### å¤šç›¸æµç®—å­ (Multiphase Flows)
- **ç•Œé¢è¿½è¸ª**: å­¦ä¹ ç•Œé¢æ¼”åŒ–ç®—å­
  - åº”ç”¨: è‡ªç”±è¡¨é¢æµã€æ°”æ¶²ä¸¤ç›¸æµ
- **ç›¸å˜è¿‡ç¨‹**: ç†”åŒ–/å‡å›ºç®—å­å»ºæ¨¡
  - åº”ç”¨: é“¸é€ å·¥è‰ºã€å¢æåˆ¶é€ 
- **å¤šå­”ä»‹è´¨**: å­¦ä¹ æœ‰æ•ˆç‰©æ€§ç®—å­
  - åº”ç”¨: æ²¹è—æ¨¡æ‹Ÿã€åœ°ä¸‹æ°´æµåŠ¨

### ç­‰ç¦»å­ä½“ç‰©ç† (Plasma Physics) ğŸŒŒ
- **Vlasov-Poisson ç³»ç»Ÿ**: åˆå§‹åˆ†å¸ƒ â†’ ç›¸ç©ºé—´æ¼”åŒ–ï¼ˆè§ `vp_system/`ï¼‰
  - åº”ç”¨: åŒæµä¸ç¨³å®šæ€§ã€ç­‰ç¦»å­ä½“æŒ¯è¡ã€æŸæµä¼ è¾“
- **ç”µç£åœºæ¼”åŒ–**: å­¦ä¹ è‡ªæ´½åœºç®—å­
  - åº”ç”¨: æ ¸èšå˜ã€ç­‰ç¦»å­ä½“åŠ é€Ÿå™¨

### ç›¸æ¯”ä¼ ç»Ÿ CFD çš„ä¼˜åŠ¿
âœ… **å¿«é€Ÿé¢„æµ‹**: è®­ç»ƒåæ¯«ç§’çº§æ±‚è§£ï¼ˆä¼ ç»Ÿ CFD: å°æ—¶/å¤©ï¼‰  
âœ… **å‚æ•°æ‰«æ**: æ— éœ€é‡æ–°è®¡ç®—ï¼Œé€‚åˆä¼˜åŒ–å’Œä¸ç¡®å®šæ€§é‡åŒ–  
âœ… **æ³›åŒ–èƒ½åŠ›**: ä¸€æ¬¡è®­ç»ƒï¼Œå¤„ç†ç›¸ä¼¼é—®é¢˜æ—  
âœ… **å¯å¾®åˆ†**: æ”¯æŒæ¢¯åº¦ä¼˜åŒ–å’Œé€†é—®é¢˜æ±‚è§£  

## ğŸ“– å‚è€ƒæ–‡çŒ® (References)

1. Lu, L., Jin, P., Pang, G., Zhang, Z., & Karniadakis, G. E. (2021). Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nature Machine Intelligence, 3(3), 218-229.

2. Wang, S., Wang, H., & Perdikaris, P. (2021). Learning the solution operator of parametric partial differential equations with physics-informed DeepONets. Science Advances, 7(40), eabi8605.

3. Lin, C., Li, Z., Lu, L., Cai, S., Maxey, M., & Karniadakis, G. E. (2021). Operator learning for predicting multiscale bubble growth dynamics. The Journal of Chemical Physics, 154(10), 104118.